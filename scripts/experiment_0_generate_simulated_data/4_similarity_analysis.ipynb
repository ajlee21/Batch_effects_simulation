{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity analysis\n",
    "\n",
    "We want to determine if the different batch simulated data is able to capture the biological signal that is present in the original data:  How much of the real input data is captured in the simulated batch data?\n",
    "\n",
    "In other words, we want to compare the representation of the real input data and the simulated batch data.  We will use **SVCCA** to compare these two representations.\n",
    "\n",
    "Here, we apply Singular Vector Canonical Correlation Analysis [Raghu et al. 2017](https://arxiv.org/pdf/1706.05806.pdf) [(github)](https://github.com/google/svcca) to the UMAP and PCA representations of our batch 1 simulated dataset vs batch n simulated datasets.  The output of the SVCCA analysis is the SVCCA mean similarity score. This single number can be interpreted as a measure of similarity between our original data vs batched dataset.\n",
    "\n",
    "Briefly, SVCCA uses Singular Value Decomposition (SVD) to extract the components explaining 99% of the variation. This is done to remove potential dimensions described by noise. Next, SVCCA performs a Canonical Correlation Analysis (CCA) on the SVD matrices to identify maximum correlations of linear combinations of both input matrices. The algorithm will identify the canonical correlations of highest magnitude across and within algorithms of the same dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/batch_effects/lib/python3.5/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/home/alexandra/anaconda3/envs/batch_effects/lib/python3.5/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/home/alexandra/anaconda3/envs/batch_effects/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import umap\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "from ggplot import *\n",
    "from functions import cca_core\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.random import seed\n",
    "randomState = 123\n",
    "seed(randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "analysis_name = 'experiment_0'\n",
    "NN_architecture = 'NN_2500_30'\n",
    "num_PCs = 100\n",
    "num_batches = [1,2,5,10,20,50,100,500,1000,2000,3000,6000]\n",
    "#num_batches = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n",
    "#num_batches = [1,2,3,4,5,6,7,8,9,10,15,20,50,100,500,800,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(),\"../..\"))\n",
    "\n",
    "simulated_data_file = os.path.join(\n",
    "    base_dir,\n",
    "    \"data\",\n",
    "    \"simulated\",\n",
    "    analysis_name,\n",
    "    \"simulated_data.txt\")\n",
    "\n",
    "batch_dir = os.path.join(\n",
    "    base_dir,\n",
    "    \"data\",\n",
    "    \"batch_simulated\",\n",
    "    analysis_name)\n",
    "\n",
    "umap_model_file = umap_model_file = os.path.join(\n",
    "    base_dir,\n",
    "    \"models\",  \n",
    "    NN_architecture,\n",
    "    \"umap_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in UMAP model\n",
    "infile = open(umap_model_file, 'rb')\n",
    "umap_model = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5539</th>\n",
       "      <th>5540</th>\n",
       "      <th>5541</th>\n",
       "      <th>5542</th>\n",
       "      <th>5543</th>\n",
       "      <th>5544</th>\n",
       "      <th>5545</th>\n",
       "      <th>5546</th>\n",
       "      <th>5547</th>\n",
       "      <th>5548</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.690384</td>\n",
       "      <td>0.642501</td>\n",
       "      <td>0.454786</td>\n",
       "      <td>0.650650</td>\n",
       "      <td>0.374857</td>\n",
       "      <td>0.414020</td>\n",
       "      <td>0.353704</td>\n",
       "      <td>0.566726</td>\n",
       "      <td>0.447497</td>\n",
       "      <td>0.165201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375559</td>\n",
       "      <td>0.600063</td>\n",
       "      <td>0.562897</td>\n",
       "      <td>0.640035</td>\n",
       "      <td>0.661894</td>\n",
       "      <td>0.325466</td>\n",
       "      <td>0.576836</td>\n",
       "      <td>0.567330</td>\n",
       "      <td>0.708088</td>\n",
       "      <td>0.615353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691060</td>\n",
       "      <td>0.655274</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.680636</td>\n",
       "      <td>0.371842</td>\n",
       "      <td>0.443242</td>\n",
       "      <td>0.374220</td>\n",
       "      <td>0.533293</td>\n",
       "      <td>0.502785</td>\n",
       "      <td>0.165815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290522</td>\n",
       "      <td>0.591411</td>\n",
       "      <td>0.609883</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.596831</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.459705</td>\n",
       "      <td>0.531669</td>\n",
       "      <td>0.710235</td>\n",
       "      <td>0.698571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.826005</td>\n",
       "      <td>0.694632</td>\n",
       "      <td>0.510346</td>\n",
       "      <td>0.642764</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.517733</td>\n",
       "      <td>0.344205</td>\n",
       "      <td>0.631357</td>\n",
       "      <td>0.663116</td>\n",
       "      <td>0.199852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589395</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.596350</td>\n",
       "      <td>0.671666</td>\n",
       "      <td>0.753091</td>\n",
       "      <td>0.160510</td>\n",
       "      <td>0.521740</td>\n",
       "      <td>0.511157</td>\n",
       "      <td>0.747738</td>\n",
       "      <td>0.728167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600721</td>\n",
       "      <td>0.564944</td>\n",
       "      <td>0.417176</td>\n",
       "      <td>0.594936</td>\n",
       "      <td>0.382868</td>\n",
       "      <td>0.440063</td>\n",
       "      <td>0.387032</td>\n",
       "      <td>0.466111</td>\n",
       "      <td>0.402363</td>\n",
       "      <td>0.223858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345520</td>\n",
       "      <td>0.547836</td>\n",
       "      <td>0.470917</td>\n",
       "      <td>0.460431</td>\n",
       "      <td>0.564647</td>\n",
       "      <td>0.256127</td>\n",
       "      <td>0.509963</td>\n",
       "      <td>0.348220</td>\n",
       "      <td>0.579351</td>\n",
       "      <td>0.583487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621544</td>\n",
       "      <td>0.615939</td>\n",
       "      <td>0.473489</td>\n",
       "      <td>0.599652</td>\n",
       "      <td>0.401605</td>\n",
       "      <td>0.481008</td>\n",
       "      <td>0.364476</td>\n",
       "      <td>0.444714</td>\n",
       "      <td>0.447605</td>\n",
       "      <td>0.215759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450535</td>\n",
       "      <td>0.532127</td>\n",
       "      <td>0.588728</td>\n",
       "      <td>0.547413</td>\n",
       "      <td>0.578212</td>\n",
       "      <td>0.234255</td>\n",
       "      <td>0.424777</td>\n",
       "      <td>0.501985</td>\n",
       "      <td>0.704726</td>\n",
       "      <td>0.717408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.735929</td>\n",
       "      <td>0.732768</td>\n",
       "      <td>0.545550</td>\n",
       "      <td>0.632979</td>\n",
       "      <td>0.460602</td>\n",
       "      <td>0.437013</td>\n",
       "      <td>0.325476</td>\n",
       "      <td>0.572898</td>\n",
       "      <td>0.682289</td>\n",
       "      <td>0.203184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414369</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.692773</td>\n",
       "      <td>0.643043</td>\n",
       "      <td>0.746655</td>\n",
       "      <td>0.215016</td>\n",
       "      <td>0.509911</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.775373</td>\n",
       "      <td>0.746010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.643262</td>\n",
       "      <td>0.647871</td>\n",
       "      <td>0.449706</td>\n",
       "      <td>0.552101</td>\n",
       "      <td>0.407846</td>\n",
       "      <td>0.421758</td>\n",
       "      <td>0.371682</td>\n",
       "      <td>0.551611</td>\n",
       "      <td>0.530684</td>\n",
       "      <td>0.185147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392800</td>\n",
       "      <td>0.503936</td>\n",
       "      <td>0.558234</td>\n",
       "      <td>0.505509</td>\n",
       "      <td>0.600357</td>\n",
       "      <td>0.213806</td>\n",
       "      <td>0.478426</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.641380</td>\n",
       "      <td>0.686138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.567826</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>0.483920</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.440530</td>\n",
       "      <td>0.442479</td>\n",
       "      <td>0.474701</td>\n",
       "      <td>0.372429</td>\n",
       "      <td>0.156235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368144</td>\n",
       "      <td>0.488229</td>\n",
       "      <td>0.461496</td>\n",
       "      <td>0.446372</td>\n",
       "      <td>0.480050</td>\n",
       "      <td>0.364032</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>0.388174</td>\n",
       "      <td>0.528046</td>\n",
       "      <td>0.492165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.746649</td>\n",
       "      <td>0.745975</td>\n",
       "      <td>0.622174</td>\n",
       "      <td>0.748726</td>\n",
       "      <td>0.455695</td>\n",
       "      <td>0.499827</td>\n",
       "      <td>0.260720</td>\n",
       "      <td>0.625723</td>\n",
       "      <td>0.640932</td>\n",
       "      <td>0.206239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421102</td>\n",
       "      <td>0.636195</td>\n",
       "      <td>0.681262</td>\n",
       "      <td>0.676396</td>\n",
       "      <td>0.637278</td>\n",
       "      <td>0.241293</td>\n",
       "      <td>0.594934</td>\n",
       "      <td>0.663525</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>0.708356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.580710</td>\n",
       "      <td>0.586180</td>\n",
       "      <td>0.419463</td>\n",
       "      <td>0.592984</td>\n",
       "      <td>0.368153</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.403313</td>\n",
       "      <td>0.445573</td>\n",
       "      <td>0.449738</td>\n",
       "      <td>0.212873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.534721</td>\n",
       "      <td>0.509361</td>\n",
       "      <td>0.491707</td>\n",
       "      <td>0.524512</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.408664</td>\n",
       "      <td>0.429760</td>\n",
       "      <td>0.594038</td>\n",
       "      <td>0.572804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.690384  0.642501  0.454786  0.650650  0.374857  0.414020  0.353704   \n",
       "1  0.691060  0.655274  0.527149  0.680636  0.371842  0.443242  0.374220   \n",
       "2  0.826005  0.694632  0.510346  0.642764  0.611429  0.517733  0.344205   \n",
       "3  0.600721  0.564944  0.417176  0.594936  0.382868  0.440063  0.387032   \n",
       "4  0.621544  0.615939  0.473489  0.599652  0.401605  0.481008  0.364476   \n",
       "5  0.735929  0.732768  0.545550  0.632979  0.460602  0.437013  0.325476   \n",
       "6  0.643262  0.647871  0.449706  0.552101  0.407846  0.421758  0.371682   \n",
       "7  0.517200  0.567826  0.363922  0.483920  0.346046  0.440530  0.442479   \n",
       "8  0.746649  0.745975  0.622174  0.748726  0.455695  0.499827  0.260720   \n",
       "9  0.580710  0.586180  0.419463  0.592984  0.368153  0.405530  0.403313   \n",
       "\n",
       "          7         8         9    ...         5539      5540      5541  \\\n",
       "0  0.566726  0.447497  0.165201    ...     0.375559  0.600063  0.562897   \n",
       "1  0.533293  0.502785  0.165815    ...     0.290522  0.591411  0.609883   \n",
       "2  0.631357  0.663116  0.199852    ...     0.589395  0.581802  0.596350   \n",
       "3  0.466111  0.402363  0.223858    ...     0.345520  0.547836  0.470917   \n",
       "4  0.444714  0.447605  0.215759    ...     0.450535  0.532127  0.588728   \n",
       "5  0.572898  0.682289  0.203184    ...     0.414369  0.576000  0.692773   \n",
       "6  0.551611  0.530684  0.185147    ...     0.392800  0.503936  0.558234   \n",
       "7  0.474701  0.372429  0.156235    ...     0.368144  0.488229  0.461496   \n",
       "8  0.625723  0.640932  0.206239    ...     0.421102  0.636195  0.681262   \n",
       "9  0.445573  0.449738  0.212873    ...     0.419540  0.534721  0.509361   \n",
       "\n",
       "       5542      5543      5544      5545      5546      5547      5548  \n",
       "0  0.640035  0.661894  0.325466  0.576836  0.567330  0.708088  0.615353  \n",
       "1  0.585498  0.596831  0.162511  0.459705  0.531669  0.710235  0.698571  \n",
       "2  0.671666  0.753091  0.160510  0.521740  0.511157  0.747738  0.728167  \n",
       "3  0.460431  0.564647  0.256127  0.509963  0.348220  0.579351  0.583487  \n",
       "4  0.547413  0.578212  0.234255  0.424777  0.501985  0.704726  0.717408  \n",
       "5  0.643043  0.746655  0.215016  0.509911  0.638528  0.775373  0.746010  \n",
       "6  0.505509  0.600357  0.213806  0.478426  0.443200  0.641380  0.686138  \n",
       "7  0.446372  0.480050  0.364032  0.477898  0.388174  0.528046  0.492165  \n",
       "8  0.676396  0.637278  0.241293  0.594934  0.663525  0.725401  0.708356  \n",
       "9  0.491707  0.524512  0.257988  0.408664  0.429760  0.594038  0.572804  \n",
       "\n",
       "[10 rows x 5549 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "simulated_data = pd.read_table(\n",
    "    simulated_data_file,\n",
    "    header=0, \n",
    "    index_col=0,\n",
    "    sep='\\t')\n",
    "\n",
    "simulated_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SVCCA score for 1 batch vs 1 batches..\n",
      "Using batch 1\n",
      "Calculating SVCCA score for 1 batch vs 2 batches..\n",
      "Using batch 2\n",
      "Calculating SVCCA score for 1 batch vs 5 batches..\n",
      "Using batch 5\n",
      "Calculating SVCCA score for 1 batch vs 10 batches..\n",
      "Using batch 10\n",
      "Calculating SVCCA score for 1 batch vs 20 batches..\n",
      "Using batch 20\n",
      "Calculating SVCCA score for 1 batch vs 50 batches..\n",
      "Using batch 50\n",
      "Calculating SVCCA score for 1 batch vs 100 batches..\n",
      "Using batch 100\n",
      "Calculating SVCCA score for 1 batch vs 500 batches..\n",
      "Using batch 500\n",
      "Calculating SVCCA score for 1 batch vs 1000 batches..\n",
      "Using batch 1000\n",
      "Calculating SVCCA score for 1 batch vs 2000 batches..\n",
      "Using batch 2000\n",
      "Calculating SVCCA score for 1 batch vs 3000 batches..\n",
      "Using batch 3000\n",
      "Calculating SVCCA score for 1 batch vs 6000 batches..\n",
      "Using batch 6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svcca_mean_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.891458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.748445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.810975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.828490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.844524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.859997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.865549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.872296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.873335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.874291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.874836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.875641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      svcca_mean_similarity\n",
       "1                  0.891458\n",
       "2                  0.748445\n",
       "5                  0.810975\n",
       "10                 0.828490\n",
       "20                 0.844524\n",
       "50                 0.859997\n",
       "100                0.865549\n",
       "500                0.872296\n",
       "1000               0.873335\n",
       "2000               0.874291\n",
       "3000               0.874836\n",
       "6000               0.875641"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Similarity using high dimensional batched data\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for i in num_batches:\n",
    "    print('Calculating SVCCA score for 1 batch vs {} batches..'.format(i))\n",
    "    \n",
    "    # Get batch 1\n",
    "    batch_1_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_1.txt\")\n",
    "\n",
    "    batch_1 = pd.read_table(\n",
    "        batch_1_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "\n",
    "    # PCA projection\n",
    "    pca = PCA(n_components=num_PCs)\n",
    "\n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    original_data_df =  batch_1\n",
    "    \n",
    "    # All batches\n",
    "    batch_other_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_\"+str(i)+\".txt\")\n",
    "\n",
    "    batch_other = pd.read_table(\n",
    "        batch_other_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "    \n",
    "    print(\"Using batch {}\".format(i))\n",
    "    \n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    batch_data_df =  batch_other\n",
    "    \n",
    "    # Check shape\n",
    "    if original_data_df.shape[0] != batch_data_df.shape[0]:\n",
    "        diff = original_data_df.shape[0] - batch_data_df.shape[0]\n",
    "        original_data_df = original_data_df.iloc[:-diff,:]\n",
    "    \n",
    "    # SVCCA\n",
    "    svcca_results = cca_core.get_cca_similarity(original_data_df.T,\n",
    "                                          batch_data_df.T,\n",
    "                                          verbose=False)\n",
    "    \n",
    "    output_list.append(np.mean(svcca_results[\"cca_coef1\"]))\n",
    "\n",
    "# Convert output to pandas dataframe\n",
    "svcca_raw_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\n",
    "svcca_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f19a26f1d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "svcca_raw_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexandra/Documents/Repos/Batch_effects_simulation/data/batch_simulated/experiment_0/Batch_1.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5539</th>\n",
       "      <th>5540</th>\n",
       "      <th>5541</th>\n",
       "      <th>5542</th>\n",
       "      <th>5543</th>\n",
       "      <th>5544</th>\n",
       "      <th>5545</th>\n",
       "      <th>5546</th>\n",
       "      <th>5547</th>\n",
       "      <th>5548</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.690384</td>\n",
       "      <td>0.642501</td>\n",
       "      <td>0.454786</td>\n",
       "      <td>0.650650</td>\n",
       "      <td>0.374857</td>\n",
       "      <td>0.414020</td>\n",
       "      <td>0.353704</td>\n",
       "      <td>0.566726</td>\n",
       "      <td>0.447497</td>\n",
       "      <td>0.165201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375559</td>\n",
       "      <td>0.600063</td>\n",
       "      <td>0.562897</td>\n",
       "      <td>0.640035</td>\n",
       "      <td>0.661894</td>\n",
       "      <td>0.325466</td>\n",
       "      <td>0.576836</td>\n",
       "      <td>0.567330</td>\n",
       "      <td>0.708088</td>\n",
       "      <td>0.615353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691060</td>\n",
       "      <td>0.655274</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.680636</td>\n",
       "      <td>0.371842</td>\n",
       "      <td>0.443242</td>\n",
       "      <td>0.374220</td>\n",
       "      <td>0.533293</td>\n",
       "      <td>0.502785</td>\n",
       "      <td>0.165815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290522</td>\n",
       "      <td>0.591411</td>\n",
       "      <td>0.609883</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.596831</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.459705</td>\n",
       "      <td>0.531669</td>\n",
       "      <td>0.710235</td>\n",
       "      <td>0.698571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.826005</td>\n",
       "      <td>0.694632</td>\n",
       "      <td>0.510346</td>\n",
       "      <td>0.642764</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.517733</td>\n",
       "      <td>0.344205</td>\n",
       "      <td>0.631357</td>\n",
       "      <td>0.663116</td>\n",
       "      <td>0.199852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589395</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.596350</td>\n",
       "      <td>0.671666</td>\n",
       "      <td>0.753091</td>\n",
       "      <td>0.160510</td>\n",
       "      <td>0.521740</td>\n",
       "      <td>0.511157</td>\n",
       "      <td>0.747738</td>\n",
       "      <td>0.728167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600721</td>\n",
       "      <td>0.564944</td>\n",
       "      <td>0.417176</td>\n",
       "      <td>0.594936</td>\n",
       "      <td>0.382868</td>\n",
       "      <td>0.440063</td>\n",
       "      <td>0.387031</td>\n",
       "      <td>0.466111</td>\n",
       "      <td>0.402363</td>\n",
       "      <td>0.223858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345520</td>\n",
       "      <td>0.547836</td>\n",
       "      <td>0.470917</td>\n",
       "      <td>0.460431</td>\n",
       "      <td>0.564647</td>\n",
       "      <td>0.256127</td>\n",
       "      <td>0.509963</td>\n",
       "      <td>0.348220</td>\n",
       "      <td>0.579351</td>\n",
       "      <td>0.583487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621544</td>\n",
       "      <td>0.615939</td>\n",
       "      <td>0.473489</td>\n",
       "      <td>0.599652</td>\n",
       "      <td>0.401605</td>\n",
       "      <td>0.481008</td>\n",
       "      <td>0.364476</td>\n",
       "      <td>0.444714</td>\n",
       "      <td>0.447605</td>\n",
       "      <td>0.215759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450535</td>\n",
       "      <td>0.532127</td>\n",
       "      <td>0.588728</td>\n",
       "      <td>0.547413</td>\n",
       "      <td>0.578212</td>\n",
       "      <td>0.234255</td>\n",
       "      <td>0.424777</td>\n",
       "      <td>0.501985</td>\n",
       "      <td>0.704726</td>\n",
       "      <td>0.717408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.735929</td>\n",
       "      <td>0.732768</td>\n",
       "      <td>0.545550</td>\n",
       "      <td>0.632979</td>\n",
       "      <td>0.460602</td>\n",
       "      <td>0.437013</td>\n",
       "      <td>0.325476</td>\n",
       "      <td>0.572898</td>\n",
       "      <td>0.682289</td>\n",
       "      <td>0.203184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414369</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.692773</td>\n",
       "      <td>0.643043</td>\n",
       "      <td>0.746655</td>\n",
       "      <td>0.215016</td>\n",
       "      <td>0.509911</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.775373</td>\n",
       "      <td>0.746010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.643262</td>\n",
       "      <td>0.647871</td>\n",
       "      <td>0.449706</td>\n",
       "      <td>0.552101</td>\n",
       "      <td>0.407846</td>\n",
       "      <td>0.421758</td>\n",
       "      <td>0.371682</td>\n",
       "      <td>0.551611</td>\n",
       "      <td>0.530684</td>\n",
       "      <td>0.185147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392800</td>\n",
       "      <td>0.503936</td>\n",
       "      <td>0.558234</td>\n",
       "      <td>0.505509</td>\n",
       "      <td>0.600357</td>\n",
       "      <td>0.213806</td>\n",
       "      <td>0.478426</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.641380</td>\n",
       "      <td>0.686138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.567826</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>0.483920</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.440530</td>\n",
       "      <td>0.442479</td>\n",
       "      <td>0.474701</td>\n",
       "      <td>0.372429</td>\n",
       "      <td>0.156235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368144</td>\n",
       "      <td>0.488229</td>\n",
       "      <td>0.461496</td>\n",
       "      <td>0.446372</td>\n",
       "      <td>0.480050</td>\n",
       "      <td>0.364032</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>0.388174</td>\n",
       "      <td>0.528046</td>\n",
       "      <td>0.492165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.746649</td>\n",
       "      <td>0.745975</td>\n",
       "      <td>0.622174</td>\n",
       "      <td>0.748726</td>\n",
       "      <td>0.455695</td>\n",
       "      <td>0.499827</td>\n",
       "      <td>0.260720</td>\n",
       "      <td>0.625723</td>\n",
       "      <td>0.640932</td>\n",
       "      <td>0.206239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421102</td>\n",
       "      <td>0.636195</td>\n",
       "      <td>0.681262</td>\n",
       "      <td>0.676396</td>\n",
       "      <td>0.637278</td>\n",
       "      <td>0.241293</td>\n",
       "      <td>0.594934</td>\n",
       "      <td>0.663525</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>0.708356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.580710</td>\n",
       "      <td>0.586180</td>\n",
       "      <td>0.419463</td>\n",
       "      <td>0.592984</td>\n",
       "      <td>0.368153</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.403313</td>\n",
       "      <td>0.445573</td>\n",
       "      <td>0.449738</td>\n",
       "      <td>0.212873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.534721</td>\n",
       "      <td>0.509361</td>\n",
       "      <td>0.491707</td>\n",
       "      <td>0.524512</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.408664</td>\n",
       "      <td>0.429760</td>\n",
       "      <td>0.594038</td>\n",
       "      <td>0.572804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.690384  0.642501  0.454786  0.650650  0.374857  0.414020  0.353704   \n",
       "1  0.691060  0.655274  0.527149  0.680636  0.371842  0.443242  0.374220   \n",
       "2  0.826005  0.694632  0.510346  0.642764  0.611429  0.517733  0.344205   \n",
       "3  0.600721  0.564944  0.417176  0.594936  0.382868  0.440063  0.387031   \n",
       "4  0.621544  0.615939  0.473489  0.599652  0.401605  0.481008  0.364476   \n",
       "5  0.735929  0.732768  0.545550  0.632979  0.460602  0.437013  0.325476   \n",
       "6  0.643262  0.647871  0.449706  0.552101  0.407846  0.421758  0.371682   \n",
       "7  0.517200  0.567826  0.363922  0.483920  0.346046  0.440530  0.442479   \n",
       "8  0.746649  0.745975  0.622174  0.748726  0.455695  0.499827  0.260720   \n",
       "9  0.580710  0.586180  0.419463  0.592984  0.368153  0.405530  0.403313   \n",
       "\n",
       "          7         8         9    ...         5539      5540      5541  \\\n",
       "0  0.566726  0.447497  0.165201    ...     0.375559  0.600063  0.562897   \n",
       "1  0.533293  0.502785  0.165815    ...     0.290522  0.591411  0.609883   \n",
       "2  0.631357  0.663116  0.199852    ...     0.589395  0.581802  0.596350   \n",
       "3  0.466111  0.402363  0.223858    ...     0.345520  0.547836  0.470917   \n",
       "4  0.444714  0.447605  0.215759    ...     0.450535  0.532127  0.588728   \n",
       "5  0.572898  0.682289  0.203184    ...     0.414369  0.576000  0.692773   \n",
       "6  0.551611  0.530684  0.185147    ...     0.392800  0.503936  0.558234   \n",
       "7  0.474701  0.372429  0.156235    ...     0.368144  0.488229  0.461496   \n",
       "8  0.625723  0.640932  0.206239    ...     0.421102  0.636195  0.681262   \n",
       "9  0.445573  0.449738  0.212873    ...     0.419540  0.534721  0.509361   \n",
       "\n",
       "       5542      5543      5544      5545      5546      5547      5548  \n",
       "0  0.640035  0.661894  0.325466  0.576836  0.567330  0.708088  0.615353  \n",
       "1  0.585498  0.596831  0.162511  0.459705  0.531669  0.710235  0.698571  \n",
       "2  0.671666  0.753091  0.160510  0.521740  0.511157  0.747738  0.728167  \n",
       "3  0.460431  0.564647  0.256127  0.509963  0.348220  0.579351  0.583487  \n",
       "4  0.547413  0.578212  0.234255  0.424777  0.501985  0.704726  0.717408  \n",
       "5  0.643043  0.746655  0.215016  0.509911  0.638528  0.775373  0.746010  \n",
       "6  0.505509  0.600357  0.213806  0.478426  0.443200  0.641380  0.686138  \n",
       "7  0.446372  0.480050  0.364032  0.477898  0.388174  0.528046  0.492165  \n",
       "8  0.676396  0.637278  0.241293  0.594934  0.663525  0.725401  0.708356  \n",
       "9  0.491707  0.524512  0.257988  0.408664  0.429760  0.594038  0.572804  \n",
       "\n",
       "[10 rows x 5549 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datasets batch 1 and original are the same\n",
    "batch_1_file = os.path.join(\n",
    "    batch_dir,\n",
    "    \"Batch_1.txt\")\n",
    "print(batch_1_file)\n",
    "\n",
    "batch_1 = pd.read_table(\n",
    "    batch_1_file,\n",
    "    header=0,\n",
    "    sep='\\t',\n",
    "    index_col=0)\n",
    "    \n",
    "batch_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexandra/Documents/Repos/Batch_effects_simulation/data/batch_simulated/experiment_0/Batch_1.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5539</th>\n",
       "      <th>5540</th>\n",
       "      <th>5541</th>\n",
       "      <th>5542</th>\n",
       "      <th>5543</th>\n",
       "      <th>5544</th>\n",
       "      <th>5545</th>\n",
       "      <th>5546</th>\n",
       "      <th>5547</th>\n",
       "      <th>5548</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.690384</td>\n",
       "      <td>0.642501</td>\n",
       "      <td>0.454786</td>\n",
       "      <td>0.650650</td>\n",
       "      <td>0.374857</td>\n",
       "      <td>0.414020</td>\n",
       "      <td>0.353704</td>\n",
       "      <td>0.566726</td>\n",
       "      <td>0.447497</td>\n",
       "      <td>0.165201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375559</td>\n",
       "      <td>0.600063</td>\n",
       "      <td>0.562897</td>\n",
       "      <td>0.640035</td>\n",
       "      <td>0.661894</td>\n",
       "      <td>0.325466</td>\n",
       "      <td>0.576836</td>\n",
       "      <td>0.567330</td>\n",
       "      <td>0.708088</td>\n",
       "      <td>0.615353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691060</td>\n",
       "      <td>0.655274</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.680636</td>\n",
       "      <td>0.371842</td>\n",
       "      <td>0.443242</td>\n",
       "      <td>0.374220</td>\n",
       "      <td>0.533293</td>\n",
       "      <td>0.502785</td>\n",
       "      <td>0.165815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290522</td>\n",
       "      <td>0.591411</td>\n",
       "      <td>0.609883</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.596831</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.459705</td>\n",
       "      <td>0.531669</td>\n",
       "      <td>0.710235</td>\n",
       "      <td>0.698571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.826005</td>\n",
       "      <td>0.694632</td>\n",
       "      <td>0.510346</td>\n",
       "      <td>0.642764</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.517733</td>\n",
       "      <td>0.344205</td>\n",
       "      <td>0.631357</td>\n",
       "      <td>0.663116</td>\n",
       "      <td>0.199852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589395</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.596350</td>\n",
       "      <td>0.671666</td>\n",
       "      <td>0.753091</td>\n",
       "      <td>0.160510</td>\n",
       "      <td>0.521740</td>\n",
       "      <td>0.511157</td>\n",
       "      <td>0.747738</td>\n",
       "      <td>0.728167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600721</td>\n",
       "      <td>0.564944</td>\n",
       "      <td>0.417176</td>\n",
       "      <td>0.594936</td>\n",
       "      <td>0.382868</td>\n",
       "      <td>0.440063</td>\n",
       "      <td>0.387031</td>\n",
       "      <td>0.466111</td>\n",
       "      <td>0.402363</td>\n",
       "      <td>0.223858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345520</td>\n",
       "      <td>0.547836</td>\n",
       "      <td>0.470917</td>\n",
       "      <td>0.460431</td>\n",
       "      <td>0.564647</td>\n",
       "      <td>0.256127</td>\n",
       "      <td>0.509963</td>\n",
       "      <td>0.348220</td>\n",
       "      <td>0.579351</td>\n",
       "      <td>0.583487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621544</td>\n",
       "      <td>0.615939</td>\n",
       "      <td>0.473489</td>\n",
       "      <td>0.599652</td>\n",
       "      <td>0.401605</td>\n",
       "      <td>0.481008</td>\n",
       "      <td>0.364476</td>\n",
       "      <td>0.444714</td>\n",
       "      <td>0.447605</td>\n",
       "      <td>0.215759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450535</td>\n",
       "      <td>0.532127</td>\n",
       "      <td>0.588728</td>\n",
       "      <td>0.547413</td>\n",
       "      <td>0.578212</td>\n",
       "      <td>0.234255</td>\n",
       "      <td>0.424777</td>\n",
       "      <td>0.501985</td>\n",
       "      <td>0.704726</td>\n",
       "      <td>0.717408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.735929</td>\n",
       "      <td>0.732768</td>\n",
       "      <td>0.545550</td>\n",
       "      <td>0.632979</td>\n",
       "      <td>0.460602</td>\n",
       "      <td>0.437013</td>\n",
       "      <td>0.325476</td>\n",
       "      <td>0.572898</td>\n",
       "      <td>0.682289</td>\n",
       "      <td>0.203184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414369</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.692773</td>\n",
       "      <td>0.643043</td>\n",
       "      <td>0.746655</td>\n",
       "      <td>0.215016</td>\n",
       "      <td>0.509911</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.775373</td>\n",
       "      <td>0.746010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.643262</td>\n",
       "      <td>0.647871</td>\n",
       "      <td>0.449706</td>\n",
       "      <td>0.552101</td>\n",
       "      <td>0.407846</td>\n",
       "      <td>0.421758</td>\n",
       "      <td>0.371682</td>\n",
       "      <td>0.551611</td>\n",
       "      <td>0.530684</td>\n",
       "      <td>0.185147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392800</td>\n",
       "      <td>0.503936</td>\n",
       "      <td>0.558234</td>\n",
       "      <td>0.505509</td>\n",
       "      <td>0.600357</td>\n",
       "      <td>0.213806</td>\n",
       "      <td>0.478426</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.641380</td>\n",
       "      <td>0.686138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.567826</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>0.483920</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.440530</td>\n",
       "      <td>0.442479</td>\n",
       "      <td>0.474701</td>\n",
       "      <td>0.372429</td>\n",
       "      <td>0.156235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368144</td>\n",
       "      <td>0.488229</td>\n",
       "      <td>0.461496</td>\n",
       "      <td>0.446372</td>\n",
       "      <td>0.480050</td>\n",
       "      <td>0.364032</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>0.388174</td>\n",
       "      <td>0.528046</td>\n",
       "      <td>0.492165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.746649</td>\n",
       "      <td>0.745975</td>\n",
       "      <td>0.622174</td>\n",
       "      <td>0.748726</td>\n",
       "      <td>0.455695</td>\n",
       "      <td>0.499827</td>\n",
       "      <td>0.260720</td>\n",
       "      <td>0.625723</td>\n",
       "      <td>0.640932</td>\n",
       "      <td>0.206239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421102</td>\n",
       "      <td>0.636195</td>\n",
       "      <td>0.681262</td>\n",
       "      <td>0.676396</td>\n",
       "      <td>0.637278</td>\n",
       "      <td>0.241293</td>\n",
       "      <td>0.594934</td>\n",
       "      <td>0.663525</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>0.708356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.580710</td>\n",
       "      <td>0.586180</td>\n",
       "      <td>0.419463</td>\n",
       "      <td>0.592984</td>\n",
       "      <td>0.368153</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.403313</td>\n",
       "      <td>0.445573</td>\n",
       "      <td>0.449738</td>\n",
       "      <td>0.212873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.534721</td>\n",
       "      <td>0.509361</td>\n",
       "      <td>0.491707</td>\n",
       "      <td>0.524512</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.408664</td>\n",
       "      <td>0.429760</td>\n",
       "      <td>0.594038</td>\n",
       "      <td>0.572804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.690384  0.642501  0.454786  0.650650  0.374857  0.414020  0.353704   \n",
       "1  0.691060  0.655274  0.527149  0.680636  0.371842  0.443242  0.374220   \n",
       "2  0.826005  0.694632  0.510346  0.642764  0.611429  0.517733  0.344205   \n",
       "3  0.600721  0.564944  0.417176  0.594936  0.382868  0.440063  0.387031   \n",
       "4  0.621544  0.615939  0.473489  0.599652  0.401605  0.481008  0.364476   \n",
       "5  0.735929  0.732768  0.545550  0.632979  0.460602  0.437013  0.325476   \n",
       "6  0.643262  0.647871  0.449706  0.552101  0.407846  0.421758  0.371682   \n",
       "7  0.517200  0.567826  0.363922  0.483920  0.346046  0.440530  0.442479   \n",
       "8  0.746649  0.745975  0.622174  0.748726  0.455695  0.499827  0.260720   \n",
       "9  0.580710  0.586180  0.419463  0.592984  0.368153  0.405530  0.403313   \n",
       "\n",
       "          7         8         9    ...         5539      5540      5541  \\\n",
       "0  0.566726  0.447497  0.165201    ...     0.375559  0.600063  0.562897   \n",
       "1  0.533293  0.502785  0.165815    ...     0.290522  0.591411  0.609883   \n",
       "2  0.631357  0.663116  0.199852    ...     0.589395  0.581802  0.596350   \n",
       "3  0.466111  0.402363  0.223858    ...     0.345520  0.547836  0.470917   \n",
       "4  0.444714  0.447605  0.215759    ...     0.450535  0.532127  0.588728   \n",
       "5  0.572898  0.682289  0.203184    ...     0.414369  0.576000  0.692773   \n",
       "6  0.551611  0.530684  0.185147    ...     0.392800  0.503936  0.558234   \n",
       "7  0.474701  0.372429  0.156235    ...     0.368144  0.488229  0.461496   \n",
       "8  0.625723  0.640932  0.206239    ...     0.421102  0.636195  0.681262   \n",
       "9  0.445573  0.449738  0.212873    ...     0.419540  0.534721  0.509361   \n",
       "\n",
       "       5542      5543      5544      5545      5546      5547      5548  \n",
       "0  0.640035  0.661894  0.325466  0.576836  0.567330  0.708088  0.615353  \n",
       "1  0.585498  0.596831  0.162511  0.459705  0.531669  0.710235  0.698571  \n",
       "2  0.671666  0.753091  0.160510  0.521740  0.511157  0.747738  0.728167  \n",
       "3  0.460431  0.564647  0.256127  0.509963  0.348220  0.579351  0.583487  \n",
       "4  0.547413  0.578212  0.234255  0.424777  0.501985  0.704726  0.717408  \n",
       "5  0.643043  0.746655  0.215016  0.509911  0.638528  0.775373  0.746010  \n",
       "6  0.505509  0.600357  0.213806  0.478426  0.443200  0.641380  0.686138  \n",
       "7  0.446372  0.480050  0.364032  0.477898  0.388174  0.528046  0.492165  \n",
       "8  0.676396  0.637278  0.241293  0.594934  0.663525  0.725401  0.708356  \n",
       "9  0.491707  0.524512  0.257988  0.408664  0.429760  0.594038  0.572804  \n",
       "\n",
       "[10 rows x 5549 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "batch_other_file = os.path.join(\n",
    "    batch_dir,\n",
    "    \"Batch_\"+str(i)+\".txt\")\n",
    "print(batch_other_file)\n",
    "\n",
    "batch_other = pd.read_table(\n",
    "    batch_other_file,\n",
    "    header=0,\n",
    "    sep='\\t',\n",
    "    index_col=0)\n",
    "\n",
    "batch_other.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8914584592316003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that batch 1 vs itself is 100% similar\n",
    "svcca_results_batch1_itself = cca_core.get_cca_similarity(batch_1.T,\n",
    "                                          batch_1.T,\n",
    "                                          verbose=False)\n",
    "np.mean(svcca_results_batch1_itself[\"cca_coef1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8914584592316003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that batch 1 vs itself is 100% similar\n",
    "svcca_results_batch1_other = cca_core.get_cca_similarity(batch_1.T,\n",
    "                                          batch_other.T,\n",
    "                                          verbose=False)\n",
    "np.mean(svcca_results_batch1_other[\"cca_coef1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SVCCA score for 1 batch vs 1 batches..\n",
      "Using batch 1\n",
      "Calculating SVCCA score for 1 batch vs 2 batches..\n",
      "Using batch 2\n",
      "Calculating SVCCA score for 1 batch vs 5 batches..\n",
      "Using batch 5\n",
      "Calculating SVCCA score for 1 batch vs 10 batches..\n",
      "Using batch 10\n",
      "Calculating SVCCA score for 1 batch vs 20 batches..\n",
      "Using batch 20\n",
      "Calculating SVCCA score for 1 batch vs 50 batches..\n",
      "Using batch 50\n",
      "Calculating SVCCA score for 1 batch vs 100 batches..\n",
      "Using batch 100\n",
      "Calculating SVCCA score for 1 batch vs 500 batches..\n",
      "Using batch 500\n",
      "Calculating SVCCA score for 1 batch vs 1000 batches..\n",
      "Using batch 1000\n",
      "Calculating SVCCA score for 1 batch vs 2000 batches..\n",
      "Using batch 2000\n",
      "Calculating SVCCA score for 1 batch vs 3000 batches..\n",
      "Using batch 3000\n",
      "Calculating SVCCA score for 1 batch vs 6000 batches..\n",
      "Using batch 6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svcca_mean_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.935091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.191038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.324309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.340191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.343503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.346443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.346655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.346389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.347476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.347351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.347427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.347503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      svcca_mean_similarity\n",
       "1                  0.935091\n",
       "2                  0.191038\n",
       "5                  0.324309\n",
       "10                 0.340191\n",
       "20                 0.343503\n",
       "50                 0.346443\n",
       "100                0.346655\n",
       "500                0.346389\n",
       "1000               0.347476\n",
       "2000               0.347351\n",
       "3000               0.347427\n",
       "6000               0.347503"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Similarity using PCA projection of batched data\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for i in num_batches:\n",
    "    print('Calculating SVCCA score for 1 batch vs {} batches..'.format(i))\n",
    "    \n",
    "    # Get batch 1\n",
    "    batch_1_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_1.txt\")\n",
    "\n",
    "    batch_1 = pd.read_table(\n",
    "        batch_1_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "\n",
    "    # PCA projection\n",
    "    pca = PCA(n_components=1000)\n",
    "\n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    original_data_PCAencoded = pca.fit_transform(batch_1)\n",
    "\n",
    "\n",
    "    original_data_PCAencoded_df = pd.DataFrame(original_data_PCAencoded,\n",
    "                                         index=batch_1.index\n",
    "                                         )\n",
    "    \n",
    "    # All batches\n",
    "    batch_other_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_\"+str(i)+\".txt\")\n",
    "\n",
    "    batch_other = pd.read_table(\n",
    "        batch_other_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "    \n",
    "    print(\"Using batch {}\".format(i))\n",
    "    \n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    batch_data_PCAencoded = pca.fit_transform(batch_other)\n",
    "    \n",
    "    \n",
    "    batch_data_PCAencoded_df = pd.DataFrame(batch_data_PCAencoded,\n",
    "                                         index=batch_other.index\n",
    "                                         )\n",
    "        \n",
    "    # Check shape\n",
    "    if original_data_PCAencoded_df.shape[0] != batch_data_PCAencoded_df.shape[0]:\n",
    "        diff = original_data_PCAencoded_df.shape[0] - batch_data_PCAencoded_df.shape[0]\n",
    "        original_data_PCAencoded_df = original_data_PCAencoded_df.iloc[:-diff,:]\n",
    "    \n",
    "    # SVCCA\n",
    "    svcca_results = cca_core.get_cca_similarity(original_data_PCAencoded_df.T,\n",
    "                                          batch_data_PCAencoded_df.T,\n",
    "                                          verbose=False)\n",
    "    \n",
    "    output_list.append(np.mean(svcca_results[\"cca_coef1\"]))\n",
    "\n",
    "# Convert output to pandas dataframe\n",
    "svcca_pca_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\n",
    "svcca_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f198eb64dd8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "svcca_pca_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SVCCA score for 1 batch vs 1 batches..\n",
      "Using batch 1\n",
      "Calculating SVCCA score for 1 batch vs 2 batches..\n",
      "Using batch 2\n",
      "Calculating SVCCA score for 1 batch vs 5 batches..\n",
      "Using batch 5\n",
      "Calculating SVCCA score for 1 batch vs 10 batches..\n",
      "Using batch 10\n",
      "Calculating SVCCA score for 1 batch vs 20 batches..\n",
      "Using batch 20\n",
      "Calculating SVCCA score for 1 batch vs 50 batches..\n",
      "Using batch 50\n",
      "Calculating SVCCA score for 1 batch vs 100 batches..\n",
      "Using batch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/batch_effects/lib/python3.5/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SVCCA score for 1 batch vs 500 batches..\n",
      "Using batch 500\n",
      "Calculating SVCCA score for 1 batch vs 1000 batches..\n",
      "Using batch 1000\n",
      "Calculating SVCCA score for 1 batch vs 2000 batches..\n",
      "Using batch 2000\n",
      "Calculating SVCCA score for 1 batch vs 3000 batches..\n",
      "Using batch 3000\n",
      "Calculating SVCCA score for 1 batch vs 6000 batches..\n",
      "Using batch 6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svcca_mean_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.745088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.748896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.748394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.745215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.773709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.746073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.747002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.742975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.745541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.741340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.743881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      svcca_mean_similarity\n",
       "1                  1.000000\n",
       "2                  0.745088\n",
       "5                  0.748896\n",
       "10                 0.748394\n",
       "20                 0.745215\n",
       "50                 0.773709\n",
       "100                0.746073\n",
       "500                0.747002\n",
       "1000               0.742975\n",
       "2000               0.745541\n",
       "3000               0.741340\n",
       "6000               0.743881"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually compute similarity by applying CCA to PCs\n",
    "from sklearn.cross_decomposition import CCA\n",
    "cca = CCA(n_components=1)\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for i in num_batches:\n",
    "    print('Calculating SVCCA score for 1 batch vs {} batches..'.format(i))\n",
    "    \n",
    "    # Get batch 1\n",
    "    batch_1_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_1.txt\")\n",
    "\n",
    "    batch_1 = pd.read_table(\n",
    "        batch_1_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "\n",
    "    # PCA projection\n",
    "    pca = PCA(n_components=1000)\n",
    "\n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    original_data_PCAencoded = pca.fit_transform(batch_1)\n",
    "\n",
    "\n",
    "    original_data_PCAencoded_df = pd.DataFrame(original_data_PCAencoded,\n",
    "                                         index=batch_1.index\n",
    "                                         )\n",
    "    \n",
    "    # All batches\n",
    "    batch_other_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_\"+str(i)+\".txt\")\n",
    "\n",
    "    batch_other = pd.read_table(\n",
    "        batch_other_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "    \n",
    "    print(\"Using batch {}\".format(i))\n",
    "    \n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    batch_data_PCAencoded = pca.fit_transform(batch_other)\n",
    "    \n",
    "    \n",
    "    batch_data_PCAencoded_df = pd.DataFrame(batch_data_PCAencoded,\n",
    "                                         index=batch_other.index\n",
    "                                         )\n",
    "        \n",
    "    # Check shape\n",
    "    if original_data_PCAencoded_df.shape[0] != batch_data_PCAencoded_df.shape[0]:\n",
    "        diff = original_data_PCAencoded_df.shape[0] - batch_data_PCAencoded_df.shape[0]\n",
    "        original_data_PCAencoded_df = original_data_PCAencoded_df.iloc[:-diff,:]\n",
    "    \n",
    "    # CCA\n",
    "    U_c, V_c = cca.fit_transform(original_data_PCAencoded_df, batch_data_PCAencoded_df)\n",
    "    result = np.corrcoef(U_c.T, V_c.T)[0,1]\n",
    "    \n",
    "    output_list.append(result)\n",
    "\n",
    "# Convert output to pandas dataframe\n",
    "pca_cca_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\n",
    "pca_cca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1987588ef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHnJJREFUeJzt3X2UFPW95/H3d3pmmBkehmEGvcqQgBviEZQAGR84bAi5bBA9LgoYH7LZKzcxxKwa1lxJJJCguOQmBrM5Hj0azGU1N7khiJsNCgpoQHdzfWDwARREHhMGUEblIcAMzHR/94+uHop5oHseYGa6Pq9z+nTVr35V/ftBz6eqf1Vdbe6OiIhEQ05nN0BERM4ehb6ISIQo9EVEIkShLyISIQp9EZEIUeiLiESIQl9EJEIU+iIiEaLQFxGJkNzObkBjZWVlPmjQoM5uhohIt7J+/fqP3L1/unpdLvQHDRpEZWVlZzdDRKRbMbO/ZFJPwzsiIhGi0BcRiRCFvohIhHS5MX2RbFNXV0dVVRW1tbWd3RTJAgUFBZSXl5OXl9em9RX6ImdYVVUVvXv3ZtCgQZhZZzdHujF35+OPP6aqqorBgwe3aRtph3fMbJGZ7Tezd1pYbmb2kJltM7MNZjYqtOwWM9saPG5pUwtFurna2lpKS0sV+NJuZkZpaWm7PjVmMqb/BDDxNMuvAoYEj+nAo0Hj+gFzgcuBy4C5ZlbS5paKdGMKfOko7X0vpQ19d38Z+OQ0Va4Ffu1JrwJ9zew84Epgtbt/4u4HgNWcfufRLsdO1PPzVVt4868HztRLiIh0ex1x9c4AYHdoviooa6m8CTObbmaVZlZZXV3dpkbUnIjz0J+2sXHPoTatLyISBR0R+s191vDTlDctdF/o7hXuXtG/f9pvEYuIdJjKykq+853vtGqdW2+9lU2bNgHJuwh89NFHbV7/xz/+cavWba+OCP0qYGBovhzYe5pyEZEuo6KigoceeqhV6/zqV79i6NChbXq9eDx+yvpnO/Q74pLNZcAdZraY5EnbQ+6+z8xWAj8OnbydAMzqgNcT6bbue+ZdNu093KHbHHp+H+b+52GnrXP06FFuuOEGqqqqiMfjzJw5k+XLl7NkyRIA1q5dy4MPPsgzzzzD888/zw9+8APi8ThlZWW8+OKLHDlyhDvvvJPKykrMjLlz5zJ16lS+/e1vs27dOmpqarj++uu57777WmzDoEGD+OpXv8qaNWuoq6tj4cKFzJo1i23btjFz5kxuu+02AH72s5+xZMkSjh8/zuTJkxu2ed1117F7925qa2uZMWMG06dPB6BXr17MmDGDZ599lsLCQv74xz9y7rnnNtuGp556ivvuu49YLEZxcTEvv/wya9euZcGCBTz77LPce++97Ny5k3379vH+++/z85//nFdffZXnnnuOAQMG8Mwzz5CXl8e4ceNYsGABFRUVp2z/dG387ne/y8qVK3nwwQeZM2cOCxYsYOnSpdTU1DBixAiGDRvGBRdcQFlZGTNmzABg9uzZnHvuua3+JHI6aUPfzH4HjAPKzKyK5BU5eQDu/hiwArga2AYcA/4xWPaJmd0PrAs2Nc/dT3dCWETOkOeff57zzz+f5cuXA3Do0CF++MMfcvToUXr27Mnvf/97brzxRqqrq/nmN7/Jyy+/zODBg/nkk+Sf7P33309xcTEbN24E4MCB5AUT8+fPp1+/fsTjccaPH8+GDRsYPnx4i+0YOHAgr7zyCnfddRfTpk3jz3/+M7W1tQwbNozbbruNVatWsXXrVl5//XXcnUmTJvHyyy8zduxYFi1aRL9+/aipqeHSSy9l6tSplJaWcvToUa644grmz5/P9773PR5//HHmzJnT7OvPmzePlStXMmDAAA4ePNhsne3bt7NmzRo2bdrE6NGjefrpp3nggQeYPHkyy5cv57rrrmuxf6dr48UXX8y8efNOqf+Tn/yEhx9+mLfeeguAXbt2MWXKFGbMmEEikWDx4sW8/vrrLb5eW6QNfXe/Oc1yB25vYdkiYFHbmiaSfdIdkZ8pl1xyCXfffTff//73ueaaa/jCF77AxIkTeeaZZ7j++utZvnw5DzzwAGvXrmXs2LENX/zp168fAC+88AKLFy9u2F5JSfID/JIlS1i4cCH19fXs27ePTZs2nTb0J02a1NCeI0eO0Lt3b3r37k1BQQEHDx5k1apVrFq1ipEjRwJw5MgRtm7dytixY3nooYf4wx/+AMDu3bvZunUrpaWl5Ofnc8011wDw+c9/ntWrV7f4+mPGjGHatGnccMMNTJkypdk6V111FXl5eVxyySXE43EmTpzY0OZdu3ad9t+5pTbGYjGmTp162nUh+WmotLSUN998kw8//JCRI0dSWlqadr3W0DdyRSLgs5/9LOvXr2fFihXMmjWLCRMmcOONN/LII4/Qr18/Lr30Unr37o27N3sdeHPlO3fuZMGCBaxbt46SkhKmTZuW9ktDPXr0ACAnJ6dhOjVfX1+PuzNr1iy+9a1vnbLe2rVreeGFF3jllVcoKipi3LhxDa+Vl5fX0LZYLEZ9fX2Lr//YY4/x2muvsXz5ckaMGNFwhN1SG8PbTrWxJadrY0FBAbFY7LT/Nim33norTzzxBB988AFf//rXM1qnNXTDNZEI2Lt3L0VFRXzta1/j7rvv5o033mDcuHG88cYbPP7449x4440AjB49mpdeeomdO3cCNAzvTJgwgYcffrhhewcOHODw4cP07NmT4uJiPvzwQ5577rl2t/PKK69k0aJFHDlyBIA9e/awf/9+Dh06RElJCUVFRbz33nu8+uqrbdr+9u3bufzyy5k3bx5lZWXs3r07/UoZamsb8/LyqKura5ifPHkyzz//POvWrePKK6/ssPal6EhfJAI2btzIzJkzG45eH330UWKxGNdccw1PPPEETz75JAD9+/dn4cKFTJkyhUQiwTnnnMPq1auZM2cOt99+OxdffDGxWIy5c+cyZcoURo4c2XACcsyYMe1u54QJE9i8eTOjR48GkidAf/Ob3zBx4kQee+wxhg8fzoUXXsgVV1zRpu3PnDmTrVu34u6MHz+ez33uc7z00kvtbjfQ5jZOnz6d4cOHM2rUKH7729+Sn5/Pl770Jfr27Zvxp4PWsOSQfNdRUVHhbfnlrI+PHOfz/+MF5l07jH8YPajjGybSRps3b+aiiy7q7GZIN5FIJBg1ahRPPfUUQ4YMabZOc+8pM1vv7hXNrhCi4R0RkS5i06ZNfOYzn2H8+PEtBn57aXhHRDrU5MmTG84JpPz0pz89I+PTLZk/fz5PPfXUKWVf+cpXmD179llrQ1sMHTqUHTt2nNHXUOiLnAUtXRWTjVKXLHam2bNnd/mAb6v2DslreEfkDCsoKODjjz9u9x+rSOpHVAoKCtq8jaw70tfflXQ15eXlVFVV0dY7yIqEpX4usa2yJvSj8tFZup+8vLw2/7SdSEfT8I6ISIQo9EVEIkShLyISIQp9EZEIUeiLiESIQl9EJEIU+iIiEaLQFxGJEIW+iEiEKPRFRCJEoS8iEiEKfRGRCFHoi4hESNaFvu5ZLiLSsqwJfd1YWUQkvawJfRERSU+hLyISIQp9EZEIUeiLiERIRqFvZhPNbIuZbTOze5pZ/mkze9HMNpjZWjMrDy2Lm9lbwWNZRzZeRERaJ+0Po5tZDHgE+DJQBawzs2XuvilUbQHwa3d/0sz+Hvhn4L8Gy2rcfUQHt1tERNogkyP9y4Bt7r7D3U8Ai4FrG9UZCrwYTK9pZrmIiHQBmYT+AGB3aL4qKAt7G5gaTE8GeptZaTBfYGaVZvaqmV3X3AuY2fSgTmV1dXUrmi8iIq2RSeg3972nxl97vRv4opm9CXwR2APUB8s+5e4VwFeBX5jZf2iyMfeF7l7h7hX9+/fPvPUiItIqacf0SR7ZDwzNlwN7wxXcfS8wBcDMegFT3f1QaBnuvsPM1gIjge3tbrmIiLRaJkf664AhZjbYzPKBm4BTrsIxszIzS21rFrAoKC8xsx6pOsAYIHwCWEREzqK0oe/u9cAdwEpgM7DE3d81s3lmNimoNg7YYmbvA+cC84Pyi4BKM3ub5AnenzS66kdERM6iTIZ3cPcVwIpGZT8KTS8Fljaz3r8Dl7SzjSIi0kGy7hu5urGyiEjLsib0TfdWFhFJK2tCX0RE0lPoi4hEiEJfRCRCFPoiIhGi0BcRiRCFvohIhCj0RUQiRKEvIhIhCn0RkQhR6IuIRIhCX0QkQhT6IiIRotAXEYmQrAt9172VRURalDWhb83+fruIiIRlTeiLiEh6Cn0RkQhR6IuIRIhCX0QkQhT6IiIRotAXEYkQhb6ISIQo9EVEIkShLyISIQp9EZEIUeiLiESIQl9EJEIyCn0zm2hmW8xsm5nd08zyT5vZi2a2wczWmll5aNktZrY1eNzSkY0XEZHWSRv6ZhYDHgGuAoYCN5vZ0EbVFgC/dvfhwDzgn4N1+wFzgcuBy4C5ZlbScc1vSndWFhFpWSZH+pcB29x9h7ufABYD1zaqMxR4MZheE1p+JbDa3T9x9wPAamBi+5vdDN1ZWUQkrUxCfwCwOzRfFZSFvQ1MDaYnA73NrDTDdUVE5CzJJPSbO4ZuPIpyN/BFM3sT+CKwB6jPcF3MbLqZVZpZZXV1dQZNEhGRtsgk9KuAgaH5cmBvuIK773X3Ke4+EpgdlB3KZN2g7kJ3r3D3iv79+7eyCyIikqlMQn8dMMTMBptZPnATsCxcwczKzCy1rVnAomB6JTDBzEqCE7gTgjIREekEaUPf3euBO0iG9WZgibu/a2bzzGxSUG0csMXM3gfOBeYH634C3E9yx7EOmBeUiYhIJ8jNpJK7rwBWNCr7UWh6KbC0hXUXcfLIX0REOpG+kSsiEiEKfRGRCFHoi4hEiEJfRCRCFPoiIhGi0BcRiRCFvohIhGRd6Lvr5soiIi3JmtA33VpZRCStrAl9ERFJT6EvIhIhCn0RkQhR6IuIRIhCX0QkQhT6IiIRotAXEYkQhb6ISIQo9EVEIkShLyISIQp9EZEIUeiLiESIQl9EJEIU+iIiEZI1oa87K4uIpJc1oS8iIukp9EVEIkShLyISIQp9EZEIUeiLiERIRqFvZhPNbIuZbTOze5pZ/ikzW2Nmb5rZBjO7OigfZGY1ZvZW8HisozsgIiKZy01XwcxiwCPAl4EqYJ2ZLXP3TaFqc4Al7v6omQ0FVgCDgmXb3X1ExzZbRETaIpMj/cuAbe6+w91PAIuBaxvVcaBPMF0M7O24JoqISEfJJPQHALtD81VBWdi9wNfMrIrkUf6doWWDg2Gfl8zsC+1prIiItE8mod/cl1290fzNwBPuXg5cDfyrmeUA+4BPuftI4LvAv5lZn0brYmbTzazSzCqrq6tb1wMREclYJqFfBQwMzZfTdPjmG8ASAHd/BSgAytz9uLt/HJSvB7YDn238Au6+0N0r3L2if//+re+FiIhkJJPQXwcMMbPBZpYP3AQsa1Tnr8B4ADO7iGToV5tZ/+BEMGZ2ATAE2NFRjRcRkdZJe/WOu9eb2R3ASiAGLHL3d81sHlDp7suAfwIeN7O7SA79THN3N7OxwDwzqwfiwG3u/skZ642IiJxW2tAHcPcVJE/Qhst+FJreBIxpZr2ngafb2cZW8cZnG0REpEHWfCPXTDdXFhFJJ2tCX0RE0lPoi4hEiEJfRCRCFPoiIhGi0BcRiRCFvohIhCj0RUQiRKEvIhIhCn0RkQhR6IuIRIhCX0QkQhT6IiIRotAXEYmQrAt9b/JLjiIikpI1oa8bK4uIpJc1oS8iIukp9EVEIkShLyISIQp9EZEIUeiLiESIQl9EJEIU+iIiEaLQFxGJEIW+iEiEKPRFRCJEoS8iEiEKfRGRCFHoi4hESEahb2YTzWyLmW0zs3uaWf4pM1tjZm+a2QYzuzq0bFaw3hYzu7IjG98c152VRURalJuugpnFgEeALwNVwDozW+bum0LV5gBL3P1RMxsKrAAGBdM3AcOA84EXzOyz7h7v6I6Y7q0sIpJWJkf6lwHb3H2Hu58AFgPXNqrjQJ9guhjYG0xfCyx29+PuvhPYFmxPREQ6QSahPwDYHZqvCsrC7gW+ZmZVJI/y72zFuiIicpZkEvrNDZw0Hjm/GXjC3cuBq4F/NbOcDNfFzKabWaWZVVZXV2fQJBERaYtMQr8KGBiaL+fk8E3KN4AlAO7+ClAAlGW4Lu6+0N0r3L2if//+mbdeRERaJZPQXwcMMbPBZpZP8sTsskZ1/gqMBzCzi0iGfnVQ7yYz62Fmg4EhwOsd1XgREWmdtFfvuHu9md0BrARiwCJ3f9fM5gGV7r4M+CfgcTO7i+TwzTR3d+BdM1sCbALqgdvPxJU7IiKSmbShD+DuK0ieoA2X/Sg0vQkY08K684H57WijiIh0EH0jV0QkQhT6IiIRotAXEYkQhb6ISIQo9EVEIkShLyISIVkX+rqzsohIy7Im9K3Z2/yIiEhY1oS+iIikp9AXEYkQhb6ISIQo9EVEIkShLyISIQp9EZEIycrQf++Dwzy3cV9nN0NEpMvJytD/l/+7k5lLN5D8HRcREUnJytA/cKyOI8frOVxT39lNERHpUrIy9A/VnACg6uCxTm6JiEjXkpWhf/BYHQB7DtR0cktERLqW7Az9mmTo7z2o0BcRCcu60HeHQ6kjfYW+iMgpsi70a+rinIgnAIW+iEhjWRf6h46daJjWmL6IyKmyJvQtuJ1+ajy/b1Eeew7WdmKLRES6nqwJ/ZTUlTtDz+vDR0eOU1sX7+QWiYh0HdkX+jUnQx90BY+ISFjWhX5qTH/YgFToa4hHRCQl60I/daQ/7PxiAPboW7kiIg2yL/SP1ZEfy2FwWU9yTFfwiIiEZV3oAxQX5ZEXy+HcPgVUaUxfRKRBRqFvZhPNbIuZbTOze5pZ/j/N7K3g8b6ZHQwti4eWLevIxrekb2EeAAP6FupErohISG66CmYWAx4BvgxUAevMbJm7b0rVcfe7QvXvBEaGNlHj7iM6rsnp9S0KQr+kkDf+euBsvrSISJeWyZH+ZcA2d9/h7ieAxcC1p6l/M/C7jmhcWxUX5gPJI/19B2uJJ/RjKiIikFnoDwB2h+argrImzOzTwGDgT6HiAjOrNLNXzey6FtabHtSprK6uzrDpLUsd6Z/ft5D6hLP/b7psU0QEMhjeAayZspYOnW8Clrp7+Guwn3L3vWZ2AfAnM9vo7ttP2Zj7QmAhQEVFRbsPyxvG9EsKgeQXtM4rLmzTthIJp7Y+Tm1dgtq6OLV1cQrzY5zTu4BYTnP/NCIiXVcmoV8FDAzNlwN7W6h7E3B7uMDd9wbPO8xsLcnx/u1NV+04qbtslvdNBv2T//4X1rxXnQztUwI8wfH6eMN04+XH6xIN22osL2b8XXEB5X2LGFBSyIC+hQwoKaQ8eD6vuJD83Ky8OCpj7k5dPLkPj/q/hUhXkUnorwOGmNlgYA/JYP9q40pmdiFQArwSKisBjrn7cTMrA8YAD3REwxsLj9un7r8zsF8RZb3yWfb2XnIMCvJiyUduDgV5MXrkxSjIy6EgN0ZZr9yTy/Ny6JF7cjq8TkFejCPH69lzsIY9B2rYc7CG/7f1Iz78Wy3h32E3g3N69wh2BkVNdgoD+hbSs0cm//xnRl08QU1dnNoTcWrqko/augQ1J5I7wZq6ODUnUuXh6USTsvB8bX2cmhOJhm2k/l8K82L0KcylT0EefQrz6FOQS3FhajqvYVlzZb0LcsmNaach0hHSpo6715vZHcBKIAYscvd3zWweUOnuqcswbwYWu4ejj4uAX5pZguT5g5+Er/rpSEX5MSZ97nx2fXyU7028EEiG/CuzxuOePDI3O3PDMSfqE3xwqJaqg8cadgZVB5I7hrd3H+T5d/Y1HPWm9C3KS+4MUjuEYOdQXlJIWa8enKhPBCEaCtfQdCqgU0Hc0vJTwjpYXt+Gk9uxHKMoL0ZBfozCvOSjID+5Q+zXM5/CvifLUssL82MkEs7h2joO19Qnn2vrqD5ynO3VR4PyOtI1p1ePXPoU5CZ3CI12Cn0Kg51FanmwLLUD6ZWfS46G4rqdRMKJuxNPOAl36hOeLAuVxxNOIgH1iQQJd+IJGspPWTfuwfKgPJ58TiSC7Ya2l6qTeq2TyyGeSCSfM1y3cVvjofrhdVN9GHJOb356/fAz+u9qp2Z056uoqPDKysrObkaHSySc/X87zp6Dx5I7g9AnhdTzsRNtuyNojtEQsAWhsG2YDs0X5OU0KUtNFwafZE5O55wS4nln6Gjb3Tl6Is6hmuQO4HBNHYdr6zlcU5csC+8wGsrqg3p1/K22/rTbN4PePXJDO4fmPlmc3GEUF51apyg/1uYDBncn4TT9Y0+cGjrxxuHRbFAE4dYodNKGYguhFG7HyXUJtpUMt0TjtjQKzObaecp2Q2Gb8FD7mwR3o3a408WiqYlYjhEzIycHYmbJ+eCRY0ZujpGTKrNGy2LJ58bLPnNOL+6dNKxN7TGz9e5eka5e540vRExOTvIcwN8VF/D5Tzdd7u4cPFbX8AnhoyPHm4R04yPoVGCf6U8xZ5qZ0atHLr165DKgb+tPuMcTzpHa5E7hlJ1Ew3TTnciuj441LDuaZmebm2MNO4acHGsSmCePAE8GfDjUurqGcGohuGI5TYMtFVjhdXNyjLxYDgV5jUKvme3GciCWk5N8DtYN12+83dxGr9vwCLepyWsRtCOn5WCOnXyN1gU33fZvTqHfRZgZJT3zKemZz8UDiju7Od1KLMcoLkoeoQ9MX72JuniCv9WGdxL1oZ3HqWVx92QAhYMilibYgumGYGshMJsNNjsZaLmNl4WONHODAG1+eQsBHCyTaFHoS+TlxZLnJPr1zO/spoiccbokQkQkQhT6IiIRotAXEYkQhb6ISIQo9EVEIkShLyISIQp9EZEIUeiLiERIl7v3jplVA39pxybKgI86qDmdKVv6AepLV5UtfcmWfkD7+vJpd++frlKXC/32MrPKTG461NVlSz9AfemqsqUv2dIPODt90fCOiEiEKPRFRCIkG0N/YWc3oINkSz9AfemqsqUv2dIPOAt9yboxfRERaVk2HumLiEgLsib0zWyimW0xs21mdk9nt6c5ZrbIzPab2Tuhsn5mttrMtgbPJUG5mdlDQX82mNmo0Dq3BPW3mtktndCPgWa2xsw2m9m7ZjajG/elwMxeN7O3g77cF5QPNrPXgnb93szyg/Iewfy2YPmg0LZmBeVbzOzKs92XoA0xM3vTzJ7tzv0I2rHLzDaa2VtmVhmUdcf3WF8zW2pm7wV/M6M7tR/u3u0fJH+wfTtwAZAPvA0M7ex2NdPOscAo4J1Q2QPAPcH0PcBPg+mrgecAA64AXgvK+wE7gueSYLrkLPfjPGBUMN0beB8Y2k37YkCvYDoPeC1o4xLgpqD8MeDbwfR/Ax4Lpm8Cfh9MDw3edz2AwcH7MdYJ77HvAv8GPBvMd8t+BG3ZBZQ1KuuO77EngVuD6Xygb2f246z/R56hf9TRwMrQ/CxgVme3q4W2DuLU0N8CnBdMnwdsCaZ/CdzcuB5wM/DLUPkp9TqpT38Evtzd+wIUAW8Al5P8gkxu4/cXsBIYHUznBvWs8XsuXO8str8ceBH4e+DZoF3drh+h195F09DvVu8xoA+wk+D8aVfoR7YM7wwAdofmq4Ky7uBcd98HEDyfE5S31Kcu1ddgWGAkySPkbtmXYEjkLWA/sJrk0e1Bd69vpl0NbQ6WHwJK6Rp9+QXwPSARzJfSPfuR4sAqM1tvZtODsu72HrsAqAb+VzDs9isz60kn9iNbQr+5X3fu7pcltdSnLtNXM+sFPA38d3c/fLqqzZR1mb64e9zdR5A8Ur4MuKi5asFzl+yLmV0D7Hf39eHiZqp26X40MsbdRwFXAbeb2djT1O2q/cklOaT7qLuPBI6SHM5pyRnvR7aEfhUwMDRfDuztpLa01odmdh5A8Lw/KG+pT12ir2aWRzLwf+vu/zso7pZ9SXH3g8BakmOpfc0st5l2NbQ5WF4MfELn92UMMMnMdgGLSQ7x/ILu148G7r43eN4P/IHkDrm7vceqgCp3fy2YX0pyJ9Bp/ciW0F8HDAmuVMgneWJqWSe3KVPLgNSZ+FtIjo+nyv8hOJt/BXAo+Bi4EphgZiXBGf8JQdlZY2YG/Auw2d1/HlrUHfvS38z6BtOFwH8CNgNrgOuDao37kurj9cCfPDnIugy4KbgqZjAwBHj97PQC3H2Wu5e7+yCS7/8/uft/oZv1I8XMeppZ79Q0yffGO3Sz95i7fwDsNrMLg6LxwKZO7UdnnKA5QydMriZ5Fcl2YHZnt6eFNv4O2AfUkdxzf4PkOOqLwNbguV9Q14BHgv5sBCpC2/k6sC14/GMn9OM/kvxouQF4K3hc3U37Mhx4M+jLO8CPgvILSIbdNuApoEdQXhDMbwuWXxDa1uygj1uAqzrxfTaOk1fvdMt+BO1+O3i8m/qb7qbvsRFAZfAe+z8kr77ptH7oG7kiIhGSLcM7IiKSAYW+iEiEKPRFRCJEoS8iEiEKfRGRCFHoi4hEiEJfRCRCFPoiIhHy/wHS2/TQsPjRawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "pca_cca_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svcca of original vs original \n",
    "# Is there an issue with the VAE generated simulated data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:batch_effects]",
   "language": "python",
   "name": "conda-env-batch_effects-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
