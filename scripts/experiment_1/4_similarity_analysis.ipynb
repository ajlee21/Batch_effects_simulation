{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity analysis\n",
    "\n",
    "We want to determine if the different batch simulated data is able to capture the biological signal that is present in the original data:  How much of the real input data is captured in the simulated batch data?\n",
    "\n",
    "In other words, we want to compare the representation of the real input data and the simulated batch data.  We will use **SVCCA** to compare these two representations.\n",
    "\n",
    "Here, we apply Singular Vector Canonical Correlation Analysis [Raghu et al. 2017](https://arxiv.org/pdf/1706.05806.pdf) [(github)](https://github.com/google/svcca) to the UMAP and PCA representations of our batch 1 simulated dataset vs batch n simulated datasets.  The output of the SVCCA analysis is the SVCCA mean similarity score. This single number can be interpreted as a measure of similarity between our original data vs batched dataset.\n",
    "\n",
    "Briefly, SVCCA uses Singular Value Decomposition (SVD) to extract the components explaining 99% of the variation. This is done to remove potential dimensions described by noise. Next, SVCCA performs a Canonical Correlation Analysis (CCA) on the SVD matrices to identify maximum correlations of linear combinations of both input matrices. The algorithm will identify the canonical correlations of highest magnitude across and within algorithms of the same dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import umap\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from ggplot import *\n",
    "from functions import cca_core\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from numpy.random import seed\n",
    "randomState = 123\n",
    "seed(randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "config_file = \"config_exp_1.txt\"\n",
    "\n",
    "d = {}\n",
    "float_params = [\"learning_rate\", \"kappa\", \"epsilon_std\"]\n",
    "str_params = [\"analysis_name\", \"NN_architecture\"]\n",
    "lst_params = [\"num_batches\"]\n",
    "with open(config_file) as f:\n",
    "    for line in f:\n",
    "        (name, val) = line.split()\n",
    "        if name in float_params:\n",
    "            d[name] = float(val)\n",
    "        elif name in str_params:\n",
    "            d[name] = str(val)\n",
    "        elif name in lst_params:\n",
    "            d[name] = ast.literal_eval(val)\n",
    "        else:\n",
    "            d[name] = int(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "analysis_name = d[\"analysis_name\"]\n",
    "NN_architecture = d[\"NN_architecture\"]\n",
    "num_PCs = d[\"num_PCs\"]\n",
    "num_batches = d[\"num_batches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(),\"../..\"))\n",
    "\n",
    "simulated_data_file = os.path.join(\n",
    "    base_dir,\n",
    "    \"data\",\n",
    "    \"simulated\",\n",
    "    analysis_name,\n",
    "    \"simulated_data.txt.xz\")\n",
    "\n",
    "batch_dir = os.path.join(\n",
    "    base_dir,\n",
    "    \"data\",\n",
    "    \"batch_simulated\",\n",
    "    analysis_name)\n",
    "\n",
    "umap_model_file = umap_model_file = os.path.join(\n",
    "    base_dir,\n",
    "    \"models\",  \n",
    "    NN_architecture,\n",
    "    \"umap_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in UMAP model\n",
    "infile = open(umap_model_file, 'rb')\n",
    "umap_model = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2490</th>\n",
       "      <th>2491</th>\n",
       "      <th>2492</th>\n",
       "      <th>2493</th>\n",
       "      <th>2494</th>\n",
       "      <th>2495</th>\n",
       "      <th>2496</th>\n",
       "      <th>2497</th>\n",
       "      <th>2498</th>\n",
       "      <th>2499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474494</td>\n",
       "      <td>0.696870</td>\n",
       "      <td>0.238944</td>\n",
       "      <td>0.267328</td>\n",
       "      <td>0.583896</td>\n",
       "      <td>0.163173</td>\n",
       "      <td>0.235312</td>\n",
       "      <td>0.575820</td>\n",
       "      <td>0.132366</td>\n",
       "      <td>0.400795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757839</td>\n",
       "      <td>0.279478</td>\n",
       "      <td>0.561519</td>\n",
       "      <td>0.491499</td>\n",
       "      <td>0.449728</td>\n",
       "      <td>0.677076</td>\n",
       "      <td>0.124337</td>\n",
       "      <td>0.541154</td>\n",
       "      <td>0.720271</td>\n",
       "      <td>0.433547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441185</td>\n",
       "      <td>0.550324</td>\n",
       "      <td>0.219221</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.590521</td>\n",
       "      <td>0.201786</td>\n",
       "      <td>0.212634</td>\n",
       "      <td>0.536296</td>\n",
       "      <td>0.177473</td>\n",
       "      <td>0.313939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810853</td>\n",
       "      <td>0.320016</td>\n",
       "      <td>0.542352</td>\n",
       "      <td>0.554965</td>\n",
       "      <td>0.502701</td>\n",
       "      <td>0.587785</td>\n",
       "      <td>0.175613</td>\n",
       "      <td>0.481203</td>\n",
       "      <td>0.498466</td>\n",
       "      <td>0.438769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.521074</td>\n",
       "      <td>0.543742</td>\n",
       "      <td>0.336719</td>\n",
       "      <td>0.252867</td>\n",
       "      <td>0.599334</td>\n",
       "      <td>0.291142</td>\n",
       "      <td>0.229665</td>\n",
       "      <td>0.567645</td>\n",
       "      <td>0.187659</td>\n",
       "      <td>0.430471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446711</td>\n",
       "      <td>0.463937</td>\n",
       "      <td>0.392560</td>\n",
       "      <td>0.507871</td>\n",
       "      <td>0.508629</td>\n",
       "      <td>0.474983</td>\n",
       "      <td>0.246932</td>\n",
       "      <td>0.588248</td>\n",
       "      <td>0.579252</td>\n",
       "      <td>0.413991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.551936</td>\n",
       "      <td>0.345861</td>\n",
       "      <td>0.302489</td>\n",
       "      <td>0.531909</td>\n",
       "      <td>0.258675</td>\n",
       "      <td>0.264847</td>\n",
       "      <td>0.422646</td>\n",
       "      <td>0.267940</td>\n",
       "      <td>0.474586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693343</td>\n",
       "      <td>0.364793</td>\n",
       "      <td>0.536226</td>\n",
       "      <td>0.499298</td>\n",
       "      <td>0.492214</td>\n",
       "      <td>0.415613</td>\n",
       "      <td>0.294030</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>0.344359</td>\n",
       "      <td>0.429401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.482794</td>\n",
       "      <td>0.633886</td>\n",
       "      <td>0.303745</td>\n",
       "      <td>0.312830</td>\n",
       "      <td>0.450920</td>\n",
       "      <td>0.218819</td>\n",
       "      <td>0.216378</td>\n",
       "      <td>0.451298</td>\n",
       "      <td>0.235343</td>\n",
       "      <td>0.466791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520269</td>\n",
       "      <td>0.356043</td>\n",
       "      <td>0.461527</td>\n",
       "      <td>0.473496</td>\n",
       "      <td>0.540051</td>\n",
       "      <td>0.382430</td>\n",
       "      <td>0.265336</td>\n",
       "      <td>0.431511</td>\n",
       "      <td>0.512582</td>\n",
       "      <td>0.530147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.426684</td>\n",
       "      <td>0.701658</td>\n",
       "      <td>0.408193</td>\n",
       "      <td>0.342460</td>\n",
       "      <td>0.431087</td>\n",
       "      <td>0.437039</td>\n",
       "      <td>0.298775</td>\n",
       "      <td>0.508043</td>\n",
       "      <td>0.216968</td>\n",
       "      <td>0.542441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577298</td>\n",
       "      <td>0.251121</td>\n",
       "      <td>0.411203</td>\n",
       "      <td>0.367234</td>\n",
       "      <td>0.420593</td>\n",
       "      <td>0.295283</td>\n",
       "      <td>0.319015</td>\n",
       "      <td>0.530860</td>\n",
       "      <td>0.503682</td>\n",
       "      <td>0.435850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.381678</td>\n",
       "      <td>0.589432</td>\n",
       "      <td>0.270064</td>\n",
       "      <td>0.342137</td>\n",
       "      <td>0.629668</td>\n",
       "      <td>0.250987</td>\n",
       "      <td>0.172229</td>\n",
       "      <td>0.401775</td>\n",
       "      <td>0.235665</td>\n",
       "      <td>0.338248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510384</td>\n",
       "      <td>0.377027</td>\n",
       "      <td>0.484328</td>\n",
       "      <td>0.492013</td>\n",
       "      <td>0.541812</td>\n",
       "      <td>0.344019</td>\n",
       "      <td>0.273659</td>\n",
       "      <td>0.545264</td>\n",
       "      <td>0.394876</td>\n",
       "      <td>0.499558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.636906</td>\n",
       "      <td>0.228519</td>\n",
       "      <td>0.464320</td>\n",
       "      <td>0.672855</td>\n",
       "      <td>0.236873</td>\n",
       "      <td>0.168115</td>\n",
       "      <td>0.330595</td>\n",
       "      <td>0.260448</td>\n",
       "      <td>0.323219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587457</td>\n",
       "      <td>0.363698</td>\n",
       "      <td>0.611703</td>\n",
       "      <td>0.467752</td>\n",
       "      <td>0.573952</td>\n",
       "      <td>0.355191</td>\n",
       "      <td>0.274361</td>\n",
       "      <td>0.565848</td>\n",
       "      <td>0.402649</td>\n",
       "      <td>0.593904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.501997</td>\n",
       "      <td>0.601793</td>\n",
       "      <td>0.210271</td>\n",
       "      <td>0.388671</td>\n",
       "      <td>0.570547</td>\n",
       "      <td>0.236643</td>\n",
       "      <td>0.168893</td>\n",
       "      <td>0.323124</td>\n",
       "      <td>0.232122</td>\n",
       "      <td>0.416512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779672</td>\n",
       "      <td>0.276626</td>\n",
       "      <td>0.620640</td>\n",
       "      <td>0.533968</td>\n",
       "      <td>0.474002</td>\n",
       "      <td>0.257428</td>\n",
       "      <td>0.229648</td>\n",
       "      <td>0.610038</td>\n",
       "      <td>0.489887</td>\n",
       "      <td>0.498294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.428152</td>\n",
       "      <td>0.623297</td>\n",
       "      <td>0.294565</td>\n",
       "      <td>0.350955</td>\n",
       "      <td>0.502915</td>\n",
       "      <td>0.277763</td>\n",
       "      <td>0.251814</td>\n",
       "      <td>0.494694</td>\n",
       "      <td>0.219805</td>\n",
       "      <td>0.450239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482579</td>\n",
       "      <td>0.388641</td>\n",
       "      <td>0.509143</td>\n",
       "      <td>0.414734</td>\n",
       "      <td>0.456965</td>\n",
       "      <td>0.407720</td>\n",
       "      <td>0.275378</td>\n",
       "      <td>0.559786</td>\n",
       "      <td>0.527550</td>\n",
       "      <td>0.514114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.474494  0.696870  0.238944  0.267328  0.583896  0.163173  0.235312   \n",
       "1  0.441185  0.550324  0.219221  0.266000  0.590521  0.201786  0.212634   \n",
       "2  0.521074  0.543742  0.336719  0.252867  0.599334  0.291142  0.229665   \n",
       "3  0.596203  0.551936  0.345861  0.302489  0.531909  0.258675  0.264847   \n",
       "4  0.482794  0.633886  0.303745  0.312830  0.450920  0.218819  0.216378   \n",
       "5  0.426684  0.701658  0.408193  0.342460  0.431087  0.437039  0.298775   \n",
       "6  0.381678  0.589432  0.270064  0.342137  0.629668  0.250987  0.172229   \n",
       "7  0.400522  0.636906  0.228519  0.464320  0.672855  0.236873  0.168115   \n",
       "8  0.501997  0.601793  0.210271  0.388671  0.570547  0.236643  0.168893   \n",
       "9  0.428152  0.623297  0.294565  0.350955  0.502915  0.277763  0.251814   \n",
       "\n",
       "          7         8         9    ...         2490      2491      2492  \\\n",
       "0  0.575820  0.132366  0.400795    ...     0.757839  0.279478  0.561519   \n",
       "1  0.536296  0.177473  0.313939    ...     0.810853  0.320016  0.542352   \n",
       "2  0.567645  0.187659  0.430471    ...     0.446711  0.463937  0.392560   \n",
       "3  0.422646  0.267940  0.474586    ...     0.693343  0.364793  0.536226   \n",
       "4  0.451298  0.235343  0.466791    ...     0.520269  0.356043  0.461527   \n",
       "5  0.508043  0.216968  0.542441    ...     0.577298  0.251121  0.411203   \n",
       "6  0.401775  0.235665  0.338248    ...     0.510384  0.377027  0.484328   \n",
       "7  0.330595  0.260448  0.323219    ...     0.587457  0.363698  0.611703   \n",
       "8  0.323124  0.232122  0.416512    ...     0.779672  0.276626  0.620640   \n",
       "9  0.494694  0.219805  0.450239    ...     0.482579  0.388641  0.509143   \n",
       "\n",
       "       2493      2494      2495      2496      2497      2498      2499  \n",
       "0  0.491499  0.449728  0.677076  0.124337  0.541154  0.720271  0.433547  \n",
       "1  0.554965  0.502701  0.587785  0.175613  0.481203  0.498466  0.438769  \n",
       "2  0.507871  0.508629  0.474983  0.246932  0.588248  0.579252  0.413991  \n",
       "3  0.499298  0.492214  0.415613  0.294030  0.536930  0.344359  0.429401  \n",
       "4  0.473496  0.540051  0.382430  0.265336  0.431511  0.512582  0.530147  \n",
       "5  0.367234  0.420593  0.295283  0.319015  0.530860  0.503682  0.435850  \n",
       "6  0.492013  0.541812  0.344019  0.273659  0.545264  0.394876  0.499558  \n",
       "7  0.467752  0.573952  0.355191  0.274361  0.565848  0.402649  0.593904  \n",
       "8  0.533968  0.474002  0.257428  0.229648  0.610038  0.489887  0.498294  \n",
       "9  0.414734  0.456965  0.407720  0.275378  0.559786  0.527550  0.514114  \n",
       "\n",
       "[10 rows x 2500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "simulated_data = pd.read_table(\n",
    "    simulated_data_file,\n",
    "    header=0, \n",
    "    index_col=0,\n",
    "    sep='\\t')\n",
    "\n",
    "simulated_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity using high dimensional (5K) batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SVCCA score for 1 batch vs 1 batches..\n",
      "Calculating SVCCA score for 1 batch vs 2 batches..\n",
      "Calculating SVCCA score for 1 batch vs 5 batches..\n",
      "Calculating SVCCA score for 1 batch vs 10 batches..\n",
      "Calculating SVCCA score for 1 batch vs 20 batches..\n",
      "Calculating SVCCA score for 1 batch vs 50 batches..\n",
      "Calculating SVCCA score for 1 batch vs 100 batches..\n",
      "Calculating SVCCA score for 1 batch vs 500 batches..\n",
      "Calculating SVCCA score for 1 batch vs 1000 batches..\n",
      "Calculating SVCCA score for 1 batch vs 2000 batches..\n",
      "Calculating SVCCA score for 1 batch vs 3000 batches..\n",
      "Calculating SVCCA score for 1 batch vs 6000 batches..\n",
      "CPU times: user 35min 15s, sys: 5min 39s, total: 40min 54s\n",
      "Wall time: 9min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate similarity using SVCCA\n",
    "\n",
    "# Store svcca scores\n",
    "output_list = []\n",
    "\n",
    "for i in num_batches:\n",
    "    print('Calculating SVCCA score for 1 batch vs {} batches..'.format(i))\n",
    "    \n",
    "    # Get batch 1\n",
    "    batch_1_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_1.txt.xz\")\n",
    "\n",
    "    batch_1 = pd.read_table(\n",
    "        batch_1_file,\n",
    "        header=0,\n",
    "        index_col=0,\n",
    "        sep='\\t')\n",
    "\n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    original_data_df =  batch_1\n",
    "    \n",
    "    # All batches\n",
    "    batch_other_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_\"+str(i)+\".txt.xz\")\n",
    "\n",
    "    batch_other = pd.read_table(\n",
    "        batch_other_file,\n",
    "        header=0,\n",
    "        index_col=0,\n",
    "        sep='\\t')\n",
    "    \n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    batch_data_df =  batch_other\n",
    "    \n",
    "    # Check shape: ensure that the number of samples is the same between the two datasets\n",
    "    if original_data_df.shape[0] != batch_data_df.shape[0]:\n",
    "        diff = original_data_df.shape[0] - batch_data_df.shape[0]\n",
    "        original_data_df = original_data_df.iloc[:-diff,:]\n",
    "    \n",
    "    # SVCCA\n",
    "    svcca_results = cca_core.get_cca_similarity(original_data_df.T,\n",
    "                                          batch_data_df.T,\n",
    "                                          verbose=False)\n",
    "    \n",
    "    output_list.append(np.mean(svcca_results[\"cca_coef1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svcca_mean_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.541352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.569670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.571708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.572152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.571929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.572532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.573037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.572765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.573110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.573160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.573498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      svcca_mean_similarity\n",
       "1                  0.995877\n",
       "2                  0.541352\n",
       "5                  0.569670\n",
       "10                 0.571708\n",
       "20                 0.572152\n",
       "50                 0.571929\n",
       "100                0.572532\n",
       "500                0.573037\n",
       "1000               0.572765\n",
       "2000               0.573110\n",
       "3000               0.573160\n",
       "6000               0.573498"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert output to pandas dataframe\n",
    "svcca_raw_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\n",
    "svcca_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2490</th>\n",
       "      <th>2491</th>\n",
       "      <th>2492</th>\n",
       "      <th>2493</th>\n",
       "      <th>2494</th>\n",
       "      <th>2495</th>\n",
       "      <th>2496</th>\n",
       "      <th>2497</th>\n",
       "      <th>2498</th>\n",
       "      <th>2499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241386</td>\n",
       "      <td>0.402257</td>\n",
       "      <td>0.248498</td>\n",
       "      <td>0.138983</td>\n",
       "      <td>0.281321</td>\n",
       "      <td>0.368983</td>\n",
       "      <td>0.415709</td>\n",
       "      <td>0.388722</td>\n",
       "      <td>0.814854</td>\n",
       "      <td>0.236955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446226</td>\n",
       "      <td>0.475811</td>\n",
       "      <td>0.305363</td>\n",
       "      <td>0.116942</td>\n",
       "      <td>0.592207</td>\n",
       "      <td>0.273776</td>\n",
       "      <td>0.195969</td>\n",
       "      <td>0.113580</td>\n",
       "      <td>0.458955</td>\n",
       "      <td>0.622206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.223560</td>\n",
       "      <td>0.509262</td>\n",
       "      <td>0.274514</td>\n",
       "      <td>0.278280</td>\n",
       "      <td>0.234895</td>\n",
       "      <td>0.374157</td>\n",
       "      <td>0.104009</td>\n",
       "      <td>0.189536</td>\n",
       "      <td>0.681980</td>\n",
       "      <td>0.352704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204581</td>\n",
       "      <td>0.186414</td>\n",
       "      <td>0.853353</td>\n",
       "      <td>0.609078</td>\n",
       "      <td>0.721962</td>\n",
       "      <td>0.507601</td>\n",
       "      <td>0.173726</td>\n",
       "      <td>0.675133</td>\n",
       "      <td>0.646617</td>\n",
       "      <td>0.462790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.290861</td>\n",
       "      <td>0.449645</td>\n",
       "      <td>0.506355</td>\n",
       "      <td>0.509598</td>\n",
       "      <td>0.555838</td>\n",
       "      <td>0.258528</td>\n",
       "      <td>0.438649</td>\n",
       "      <td>0.377589</td>\n",
       "      <td>0.455247</td>\n",
       "      <td>0.493701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399989</td>\n",
       "      <td>0.232601</td>\n",
       "      <td>0.414148</td>\n",
       "      <td>0.238453</td>\n",
       "      <td>0.201358</td>\n",
       "      <td>0.680870</td>\n",
       "      <td>0.308055</td>\n",
       "      <td>0.379185</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>0.330099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.182675</td>\n",
       "      <td>0.685023</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.276972</td>\n",
       "      <td>0.442969</td>\n",
       "      <td>0.339968</td>\n",
       "      <td>0.386469</td>\n",
       "      <td>0.261094</td>\n",
       "      <td>0.174565</td>\n",
       "      <td>0.602919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312858</td>\n",
       "      <td>0.422406</td>\n",
       "      <td>0.405815</td>\n",
       "      <td>0.711816</td>\n",
       "      <td>0.642572</td>\n",
       "      <td>0.415431</td>\n",
       "      <td>0.283052</td>\n",
       "      <td>0.274896</td>\n",
       "      <td>0.364454</td>\n",
       "      <td>0.436084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359666</td>\n",
       "      <td>0.442325</td>\n",
       "      <td>0.418003</td>\n",
       "      <td>0.304205</td>\n",
       "      <td>0.379381</td>\n",
       "      <td>0.319710</td>\n",
       "      <td>0.415753</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>0.501125</td>\n",
       "      <td>0.243813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323731</td>\n",
       "      <td>0.557872</td>\n",
       "      <td>0.482986</td>\n",
       "      <td>0.425119</td>\n",
       "      <td>0.625209</td>\n",
       "      <td>0.759142</td>\n",
       "      <td>0.532774</td>\n",
       "      <td>0.394323</td>\n",
       "      <td>0.522555</td>\n",
       "      <td>0.442546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.241386  0.402257  0.248498  0.138983  0.281321  0.368983  0.415709   \n",
       "1  0.223560  0.509262  0.274514  0.278280  0.234895  0.374157  0.104009   \n",
       "2  0.290861  0.449645  0.506355  0.509598  0.555838  0.258528  0.438649   \n",
       "3  0.182675  0.685023  0.449600  0.276972  0.442969  0.339968  0.386469   \n",
       "4  0.359666  0.442325  0.418003  0.304205  0.379381  0.319710  0.415753   \n",
       "\n",
       "          7         8         9    ...         2490      2491      2492  \\\n",
       "0  0.388722  0.814854  0.236955    ...     0.446226  0.475811  0.305363   \n",
       "1  0.189536  0.681980  0.352704    ...     0.204581  0.186414  0.853353   \n",
       "2  0.377589  0.455247  0.493701    ...     0.399989  0.232601  0.414148   \n",
       "3  0.261094  0.174565  0.602919    ...     0.312858  0.422406  0.405815   \n",
       "4  0.542512  0.501125  0.243813    ...     0.323731  0.557872  0.482986   \n",
       "\n",
       "       2493      2494      2495      2496      2497      2498      2499  \n",
       "0  0.116942  0.592207  0.273776  0.195969  0.113580  0.458955  0.622206  \n",
       "1  0.609078  0.721962  0.507601  0.173726  0.675133  0.646617  0.462790  \n",
       "2  0.238453  0.201358  0.680870  0.308055  0.379185  0.380219  0.330099  \n",
       "3  0.711816  0.642572  0.415431  0.283052  0.274896  0.364454  0.436084  \n",
       "4  0.425119  0.625209  0.759142  0.532774  0.394323  0.522555  0.442546  \n",
       "\n",
       "[5 rows x 2500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permute simulated data\n",
    "shuffled_simulated_arr = []\n",
    "num_samples = simulated_data.shape[0]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    row = list(simulated_data.values[i])\n",
    "    shuffled_simulated_row = random.sample(row, len(row))\n",
    "    shuffled_simulated_arr.append(shuffled_simulated_row)\n",
    "\n",
    "shuffled_simulated_data = pd.DataFrame(shuffled_simulated_arr, index=simulated_data.index, columns=simulated_data.columns)\n",
    "shuffled_simulated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5798155477292326\n",
      "CPU times: user 2min 37s, sys: 21.8 s, total: 2min 58s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVCCA\n",
    "svcca_results = cca_core.get_cca_similarity(simulated_data.T,\n",
    "                                      shuffled_simulated_data.T,\n",
    "                                      verbose=False)\n",
    "\n",
    "print(np.mean(svcca_results[\"cca_coef1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f927c526668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "svcca_raw_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity using PCA projection of batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noutput_list = []\\n\\nfor i in num_batches:\\n    print(\\'Calculating SVCCA score for 1 batch vs {} batches..\\'.format(i))\\n    \\n    # Get batch 1\\n    batch_1_file = os.path.join(\\n        batch_dir,\\n        \"Batch_1.txt\")\\n\\n    batch_1 = pd.read_table(\\n        batch_1_file,\\n        header=0,\\n        sep=\\'\\t\\',\\n        index_col=0)\\n\\n    # PCA projection\\n    pca = PCA(n_components=num_PCs)\\n\\n    # Use trained model to encode expression data into SAME latent space\\n    original_data_PCAencoded = pca.fit_transform(batch_1)\\n\\n\\n    original_data_PCAencoded_df = pd.DataFrame(original_data_PCAencoded,\\n                                         index=batch_1.index\\n                                         )\\n    \\n    # All batches\\n    batch_other_file = os.path.join(\\n        batch_dir,\\n        \"Batch_\"+str(i)+\".txt\")\\n\\n    batch_other = pd.read_table(\\n        batch_other_file,\\n        header=0,\\n        sep=\\'\\t\\',\\n        index_col=0)\\n    \\n    print(\"Using batch {}\".format(i))\\n    \\n    # Use trained model to encode expression data into SAME latent space\\n    batch_data_PCAencoded = pca.fit_transform(batch_other)\\n    \\n    \\n    batch_data_PCAencoded_df = pd.DataFrame(batch_data_PCAencoded,\\n                                         index=batch_other.index\\n                                         )\\n        \\n    # Check shape\\n    if original_data_PCAencoded_df.shape[0] != batch_data_PCAencoded_df.shape[0]:\\n        diff = original_data_PCAencoded_df.shape[0] - batch_data_PCAencoded_df.shape[0]\\n        original_data_PCAencoded_df = original_data_PCAencoded_df.iloc[:-diff,:]\\n    \\n    # SVCCA\\n    svcca_results = cca_core.get_cca_similarity(original_data_PCAencoded_df.T,\\n                                          batch_data_PCAencoded_df.T,\\n                                          verbose=False)\\n    \\n    output_list.append(np.mean(svcca_results[\"cca_coef1\"]))\\n\\n# Convert output to pandas dataframe\\nsvcca_pca_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\\nsvcca_pca_df\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "output_list = []\n",
    "\n",
    "for i in num_batches:\n",
    "    print('Calculating SVCCA score for 1 batch vs {} batches..'.format(i))\n",
    "    \n",
    "    # Get batch 1\n",
    "    batch_1_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_1.txt\")\n",
    "\n",
    "    batch_1 = pd.read_table(\n",
    "        batch_1_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "\n",
    "    # PCA projection\n",
    "    pca = PCA(n_components=num_PCs)\n",
    "\n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    original_data_PCAencoded = pca.fit_transform(batch_1)\n",
    "\n",
    "\n",
    "    original_data_PCAencoded_df = pd.DataFrame(original_data_PCAencoded,\n",
    "                                         index=batch_1.index\n",
    "                                         )\n",
    "    \n",
    "    # All batches\n",
    "    batch_other_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_\"+str(i)+\".txt\")\n",
    "\n",
    "    batch_other = pd.read_table(\n",
    "        batch_other_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "    \n",
    "    print(\"Using batch {}\".format(i))\n",
    "    \n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    batch_data_PCAencoded = pca.fit_transform(batch_other)\n",
    "    \n",
    "    \n",
    "    batch_data_PCAencoded_df = pd.DataFrame(batch_data_PCAencoded,\n",
    "                                         index=batch_other.index\n",
    "                                         )\n",
    "        \n",
    "    # Check shape\n",
    "    if original_data_PCAencoded_df.shape[0] != batch_data_PCAencoded_df.shape[0]:\n",
    "        diff = original_data_PCAencoded_df.shape[0] - batch_data_PCAencoded_df.shape[0]\n",
    "        original_data_PCAencoded_df = original_data_PCAencoded_df.iloc[:-diff,:]\n",
    "    \n",
    "    # SVCCA\n",
    "    svcca_results = cca_core.get_cca_similarity(original_data_PCAencoded_df.T,\n",
    "                                          batch_data_PCAencoded_df.T,\n",
    "                                          verbose=False)\n",
    "    \n",
    "    output_list.append(np.mean(svcca_results[\"cca_coef1\"]))\n",
    "\n",
    "# Convert output to pandas dataframe\n",
    "svcca_pca_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\n",
    "svcca_pca_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Plot\\nsvcca_pca_df.plot()'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Plot\n",
    "svcca_pca_df.plot()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually compute similarity by applying CCA to PC batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncca = CCA(n_components=1)\\n\\noutput_list = []\\n\\nfor i in num_batches:\\n    print(\\'Calculating SVCCA score for 1 batch vs {} batches..\\'.format(i))\\n    \\n    # Get batch 1\\n    batch_1_file = os.path.join(\\n        batch_dir,\\n        \"Batch_1.txt\")\\n\\n    batch_1 = pd.read_table(\\n        batch_1_file,\\n        header=0,\\n        sep=\\'\\t\\',\\n        index_col=0)\\n\\n    # PCA projection\\n    pca = PCA(n_components=num_PCs)\\n\\n    # Use trained model to encode expression data into SAME latent space\\n    original_data_PCAencoded = pca.fit_transform(batch_1)\\n\\n\\n    original_data_PCAencoded_df = pd.DataFrame(original_data_PCAencoded,\\n                                         index=batch_1.index\\n                                         )\\n    \\n    # All batches\\n    batch_other_file = os.path.join(\\n        batch_dir,\\n        \"Batch_\"+str(i)+\".txt\")\\n\\n    batch_other = pd.read_table(\\n        batch_other_file,\\n        header=0,\\n        sep=\\'\\t\\',\\n        index_col=0)\\n    \\n    print(\"Using batch {}\".format(i))\\n    \\n    # Use trained model to encode expression data into SAME latent space\\n    batch_data_PCAencoded = pca.fit_transform(batch_other)\\n    \\n    \\n    batch_data_PCAencoded_df = pd.DataFrame(batch_data_PCAencoded,\\n                                         index=batch_other.index\\n                                         )\\n        \\n    # Check shape\\n    if original_data_PCAencoded_df.shape[0] != batch_data_PCAencoded_df.shape[0]:\\n        diff = original_data_PCAencoded_df.shape[0] - batch_data_PCAencoded_df.shape[0]\\n        original_data_PCAencoded_df = original_data_PCAencoded_df.iloc[:-diff,:]\\n    \\n    # CCA\\n    U_c, V_c = cca.fit_transform(original_data_PCAencoded_df, batch_data_PCAencoded_df)\\n    result = np.corrcoef(U_c.T, V_c.T)[0,1]\\n    \\n    output_list.append(result)\\n\\n# Convert output to pandas dataframe\\npca_cca_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\\npca_cca_df\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cca = CCA(n_components=1)\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for i in num_batches:\n",
    "    print('Calculating SVCCA score for 1 batch vs {} batches..'.format(i))\n",
    "    \n",
    "    # Get batch 1\n",
    "    batch_1_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_1.txt\")\n",
    "\n",
    "    batch_1 = pd.read_table(\n",
    "        batch_1_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "\n",
    "    # PCA projection\n",
    "    pca = PCA(n_components=num_PCs)\n",
    "\n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    original_data_PCAencoded = pca.fit_transform(batch_1)\n",
    "\n",
    "\n",
    "    original_data_PCAencoded_df = pd.DataFrame(original_data_PCAencoded,\n",
    "                                         index=batch_1.index\n",
    "                                         )\n",
    "    \n",
    "    # All batches\n",
    "    batch_other_file = os.path.join(\n",
    "        batch_dir,\n",
    "        \"Batch_\"+str(i)+\".txt\")\n",
    "\n",
    "    batch_other = pd.read_table(\n",
    "        batch_other_file,\n",
    "        header=0,\n",
    "        sep='\\t',\n",
    "        index_col=0)\n",
    "    \n",
    "    print(\"Using batch {}\".format(i))\n",
    "    \n",
    "    # Use trained model to encode expression data into SAME latent space\n",
    "    batch_data_PCAencoded = pca.fit_transform(batch_other)\n",
    "    \n",
    "    \n",
    "    batch_data_PCAencoded_df = pd.DataFrame(batch_data_PCAencoded,\n",
    "                                         index=batch_other.index\n",
    "                                         )\n",
    "        \n",
    "    # Check shape\n",
    "    if original_data_PCAencoded_df.shape[0] != batch_data_PCAencoded_df.shape[0]:\n",
    "        diff = original_data_PCAencoded_df.shape[0] - batch_data_PCAencoded_df.shape[0]\n",
    "        original_data_PCAencoded_df = original_data_PCAencoded_df.iloc[:-diff,:]\n",
    "    \n",
    "    # CCA\n",
    "    U_c, V_c = cca.fit_transform(original_data_PCAencoded_df, batch_data_PCAencoded_df)\n",
    "    result = np.corrcoef(U_c.T, V_c.T)[0,1]\n",
    "    \n",
    "    output_list.append(result)\n",
    "\n",
    "# Convert output to pandas dataframe\n",
    "pca_cca_df = pd.DataFrame(output_list, columns=[\"svcca_mean_similarity\"], index=num_batches)\n",
    "pca_cca_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Plot\\npca_cca_df.plot()'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Plot\n",
    "pca_cca_df.plot()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:batch_effects]",
   "language": "python",
   "name": "conda-env-batch_effects-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
